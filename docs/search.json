[
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html",
    "title": "SARIMAã‚’ä½¿ã£ãŸåŒ»ç™‚è²»äºˆæ¸¬ã®ãƒ‡ãƒ¢",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport warnings\n\n# è­¦å‘Šã‚’ç„¡è¦–ï¼ˆåæŸã«é–¢ã™ã‚‹è­¦å‘Šãªã©ãŒå‡ºã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ï¼‰\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™",
    "title": "SARIMAã‚’ä½¿ã£ãŸåŒ»ç™‚è²»äºˆæ¸¬ã®ãƒ‡ãƒ¢",
    "section": "1. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™",
    "text": "1. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n\n# 1. ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ\nnp.random.seed(42)\n\n# æœŸé–“: éå»10å¹´åˆ† (120ãƒ¶æœˆ)\ndates = pd.date_range(start='2015-01-01', periods=120, freq='MS')\n\n# ãƒˆãƒ¬ãƒ³ãƒ‰: å¾ã€…ã«åŒ»ç™‚è²»ãŒä¸ŠãŒã£ã¦ã„ã (ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰)\ntrend = np.linspace(100, 150, 120)  # 100å˜ä½ã‹ã‚‰1000å˜ä½ã¸ä¸Šæ˜‡ &lt;- ã“ã“ã‚’å¤‰ãˆãŸã€‚\n\n# å­£ç¯€æ€§: 12ãƒ¶æœˆå‘¨æœŸ (ä¾‹: å†¬ã«é«˜ãã€å¤ã«ä½ã„ãªã©)\nseasonality = 10 * np.sin(np.linspace(0, 20 * np.pi, 120))\n\n# ãƒã‚¤ã‚º: ãƒ©ãƒ³ãƒ€ãƒ ãªå¤‰å‹•\nnoise = np.random.normal(scale=3, size=120)\n\n# åˆæˆã—ã¦ã€Œæœˆæ¬¡åŒ»ç™‚è²»ãƒ‡ãƒ¼ã‚¿ã€ã¨ã™ã‚‹\nmedical_costs = trend + seasonality + noise\n\n# DataFrameåŒ–\ndf = pd.DataFrame({'Date': dates, 'Cost (100 million)': medical_costs})\ndf.set_index('Date', inplace=True)\ndf[\"Cost (100 million)\"]  = df[\"Cost (100 million)\"].round(2)\n\nprint(\"ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5è¡Œ:\")\ndisplay(df.head())\n\nãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5è¡Œ:\n\n\n\n\n\n\n\n\n\nCost (100 million)\n\n\nDate\n\n\n\n\n\n2015-01-01\n101.49\n\n\n2015-02-01\n187.82\n\n\n2015-03-01\n277.03\n\n\n2015-04-01\n364.15\n\n\n2015-05-01\n440.64"
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–-plotly",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–-plotly",
    "title": "SARIMAã‚’ä½¿ã£ãŸåŒ»ç™‚è²»äºˆæ¸¬ã®ãƒ‡ãƒ¢",
    "section": "å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ– (Plotly)",
    "text": "å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ– (Plotly)\n\n# 2. å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=df.index,\n    y=df['Cost (100 million)'],\n    mode='lines+markers',\n    name='å®Ÿç¸¾å€¤',\n    line=dict(color='blue')\n))\n\nfig.update_layout(\n    title='åŒ»ç™‚è²»ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆéå»10å¹´ï¼‰',\n    xaxis_title='å¹´æœˆ',\n    yaxis_title='åŒ»ç™‚è²»ï¼ˆ1å„„å††ï¼‰',\n    template='plotly_white'\n)\n\nfig.show(renderer=\"notebook\")"
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#sarimaãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨å­¦ç¿’",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#sarimaãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨å­¦ç¿’",
    "title": "SARIMAã‚’ä½¿ã£ãŸåŒ»ç™‚è²»äºˆæ¸¬ã®ãƒ‡ãƒ¢",
    "section": "SARIMAãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨å­¦ç¿’",
    "text": "SARIMAãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨å­¦ç¿’\n\\(\\phi_p(B) \\Phi_P(B^s) (1 - B)^d (1 - B^s)^D y_t = \\theta_q(B) \\Theta_Q(B^s) \\varepsilon_t\\)\nSARIMAãƒ¢ãƒ‡ãƒ«ï¼ˆå­£ç¯€æ€§è‡ªå·±å›å¸°å’Œåˆ†ç§»å‹•å¹³å‡ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚\nâ€»æœ¬æ¥ã¯AICãªã©ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚’è¡Œã„ã¾ã™ãŒã€ä»Šå›ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã«åˆã‚ã›ã¦ä¸€èˆ¬çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \\((p, d, q) \\times (P, D, Q, s)\\) ã‚’è¨­å®šã—ã¾ã™ã€‚\n\n# 3. SARIMAãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰\n# order=(p, d, q), seasonal_order=(P, D, Q, s)\n# å‘¨æœŸ s=12 (æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚)\nsarima_model = SARIMAX(\n    df['Cost (100 million)'],\n    order=(1, 1, 1),\n    seasonal_order=(1, 1, 1, 12),\n    enforce_stationarity=False,\n    enforce_invertibility=False\n)\n\n# ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’\nsarima_result = sarima_model.fit()\n\nprint(sarima_result.summary())\n\n                                     SARIMAX Results                                      \n==========================================================================================\nDep. Variable:                 Cost (100 million)   No. Observations:                  120\nModel:             SARIMAX(1, 1, 1)x(1, 1, 1, 12)   Log Likelihood                -244.124\nDate:                            Sun, 14 Dec 2025   AIC                            498.249\nTime:                                    15:21:45   BIC                            510.912\nSample:                                01-01-2015   HQIC                           503.362\n                                     - 12-01-2024                                         \nCovariance Type:                              opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.0269      0.121      0.223      0.824      -0.210       0.263\nma.L1         -1.0000    238.268     -0.004      0.997    -467.996     465.996\nar.S.L12      -0.2240      0.144     -1.554      0.120      -0.507       0.058\nma.S.L12      -0.5145      0.166     -3.101      0.002      -0.840      -0.189\nsigma2        10.2597   2444.753      0.004      0.997   -4781.367    4801.887\n===================================================================================\nLjung-Box (L1) (Q):                   0.01   Jarque-Bera (JB):                 0.43\nProb(Q):                              0.91   Prob(JB):                         0.81\nHeteroskedasticity (H):               0.95   Skew:                             0.08\nProb(H) (two-sided):                  0.88   Kurtosis:                         3.29\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step)."
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#å‘ã“ã†5å¹´é–“ã®äºˆæ¸¬ã¨å¯è¦–åŒ–",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#å‘ã“ã†5å¹´é–“ã®äºˆæ¸¬ã¨å¯è¦–åŒ–",
    "title": "SARIMAã‚’ä½¿ã£ãŸåŒ»ç™‚è²»äºˆæ¸¬ã®ãƒ‡ãƒ¢",
    "section": "å‘ã“ã†5å¹´é–“ã®äºˆæ¸¬ã¨å¯è¦–åŒ–",
    "text": "å‘ã“ã†5å¹´é–“ã®äºˆæ¸¬ã¨å¯è¦–åŒ–\n\n# 4. å‘ã“ã†5å¹´é–“ (60ãƒ¶æœˆ) ã®äºˆæ¸¬\nforecast_steps = 60\npred_uc = sarima_result.get_forecast(steps=forecast_steps)\n\n# äºˆæ¸¬å€¤ï¼ˆæœŸå¾…å€¤ï¼‰\npred_mean = pred_uc.predicted_mean\n\n# 95%ä¿¡é ¼åŒºé–“\npred_ci = pred_uc.conf_int()\n\n# --- å¯è¦–åŒ– ---\nfig_forecast = go.Figure()\n\n# 1. å®Ÿç¸¾å€¤ã®ãƒ—ãƒ­ãƒƒãƒˆ\nfig_forecast.add_trace(go.Scatter(\n    x=df.index,\n    y=df['Cost (100 million)'],\n    mode='lines',\n    name='å®Ÿç¸¾å€¤',\n    line=dict(color='blue')\n))\n\n# 2. äºˆæ¸¬å€¤ã®ãƒ—ãƒ­ãƒƒãƒˆ\nfig_forecast.add_trace(go.Scatter(\n    x=pred_mean.index,\n    y=pred_mean,\n    mode='lines',\n    name='äºˆæ¸¬å€¤ (ä»Šå¾Œ5å¹´)',\n    line=dict(color='red', dash='dash')\n))\n\n# 3. ä¿¡é ¼åŒºé–“ã®ãƒ—ãƒ­ãƒƒãƒˆ (ä¸Šé™ã¨ä¸‹é™ã®é–“ã‚’å¡—ã‚Šã¤ã¶ã™)\nfig_forecast.add_trace(go.Scatter(\n    x=pred_ci.index,\n    y=pred_ci.iloc[:, 0], # ä¸‹é™\n    mode='lines',\n    line=dict(width=0),\n    showlegend=False,\n    name='Lower Bound'\n))\n\nfig_forecast.add_trace(go.Scatter(\n    x=pred_ci.index,\n    y=pred_ci.iloc[:, 1], # ä¸Šé™\n    mode='lines',\n    line=dict(width=0),\n    fill='tonexty', # ã²ã¨ã¤å‰ã®ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆä¸‹é™ï¼‰ã¨ã®é–“ã‚’åŸ‹ã‚ã‚‹\n    fillcolor='rgba(255, 0, 0, 0.2)', # èµ¤è‰²ã®åŠé€æ˜\n    name='95%ä¿¡é ¼åŒºé–“'\n))\n\nfig_forecast.update_layout(\n    title='SARIMAãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹åŒ»ç™‚è²»äºˆæ¸¬ï¼ˆå‘ã“ã†5å¹´ï¼‰',\n    xaxis_title='å¹´æœˆ',\n    yaxis_title='åŒ»ç™‚è²»',\n    template='plotly_white',\n    hovermode=\"x unified\"\n)\n\nfig_forecast.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#åŒ»ç™‚è²»ä¸Šæ˜‡é‡‘é¡ã®ç®—å‡º",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#åŒ»ç™‚è²»ä¸Šæ˜‡é‡‘é¡ã®ç®—å‡º",
    "title": "SARIMAã‚’ä½¿ã£ãŸåŒ»ç™‚è²»äºˆæ¸¬ã®ãƒ‡ãƒ¢",
    "section": "åŒ»ç™‚è²»ä¸Šæ˜‡é‡‘é¡ã®ç®—å‡º",
    "text": "åŒ»ç™‚è²»ä¸Šæ˜‡é‡‘é¡ã®ç®—å‡º\n\n# 5. ä¸Šæ˜‡é‡‘é¡ã®ç®—å‡º\n\n# ç›´è¿‘1å¹´é–“ã®å®Ÿç¸¾åˆè¨ˆ\nlast_year_actual = df['Cost (100 million)'].iloc[-12:].sum()\n\n# 5å¹´å¾Œï¼ˆäºˆæ¸¬æœŸé–“ã®æœ€å¾Œã®1å¹´é–“ï¼‰ã®äºˆæ¸¬åˆè¨ˆ\nlast_year_forecast = pred_mean.iloc[-12:].sum()\n\n# ä¸Šæ˜‡é¡ã¨ä¸Šæ˜‡ç‡\nincrease_amount = last_year_forecast - last_year_actual\nincrease_rate = (increase_amount / last_year_actual) * 100\n\nprint(f\"--- åŒ»ç™‚è²»ä¸Šæ˜‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ---\")\nprint(f\"ç›´è¿‘1å¹´é–“ã®åŒ»ç™‚è²»åˆè¨ˆ: {last_year_actual:,.2f}\")\nprint(f\"5å¹´å¾Œã®å¹´é–“åŒ»ç™‚è²»åˆè¨ˆ: {last_year_forecast:,.2f}\")\nprint(f\"--------------------------------\")\nprint(f\"å¹´é–“å¢—åŠ é¡: +{increase_amount:,.2f}\")\nprint(f\"ä¸Šæ˜‡ç‡: {increase_rate:.2f}%\")\n\n--- åŒ»ç™‚è²»ä¸Šæ˜‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ---\nç›´è¿‘1å¹´é–“ã®åŒ»ç™‚è²»åˆè¨ˆ: 1,777.19\n5å¹´å¾Œã®å¹´é–“åŒ»ç™‚è²»åˆè¨ˆ: 2,088.13\n--------------------------------\nå¹´é–“å¢—åŠ é¡: +310.94\nä¸Šæ˜‡ç‡: 17.50%"
  },
  {
    "objectID": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html",
    "href": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html",
    "title": "NumPyroã‚’ä½¿ã£ã¦ãƒ™ã‚¤ã‚ºå›å¸°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…",
    "section": "",
    "text": "å‚ç…§ã•ã›ã¦ã„ãŸã ã„ãŸè¨˜äº‹:\nã€ãƒ™ã‚¤ã‚ºçµ±è¨ˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°å…¥é–€ã€‘NumPyroã§å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ã¤ãã‚‹\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport jax.numpy as jnp\nfrom jax import random, vmap, grad, jit, lax\n\nimport numpyro\nfrom numpyro import plate, sample\nfrom numpyro.infer import MCMC, NUTS, Predictive\nimport numpyro.distributions as dist\n# ãƒãƒƒãƒ—ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\ndf = sns.load_dataset(\"tips\")\n\n# ä¼šè¨ˆç·é¡ã¨ãƒãƒƒãƒ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’å¯è¦–åŒ–\nsns.set(style=\"darkgrid\")\nsns.jointplot(\n    x=\"total_bill\",\n    y=\"tip\",\n    data=df,\n    kind=\"scatter\",\n    xlim=(0, 60),\n    ylim=(0, 12),\n    color=\"b\",\n    height=7,\n)\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4"
  },
  {
    "objectID": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html#ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«-",
    "href": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html#ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«-",
    "title": "NumPyroã‚’ä½¿ã£ã¦ãƒ™ã‚¤ã‚ºå›å¸°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…",
    "section": "-ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«-",
    "text": "-ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«-\n\n\n\nimage.png\n\n\n\n# ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆ\ndef model(X, Y=None):\n    pm_a = numpyro.sample(\"pm_a\", dist.Normal(0, 10))\n    pm_b = numpyro.sample(\"pm_b\", dist.Normal(0, 10))\n\n    mu = pm_a * X + pm_b\n    sigma = 1.0\n\n    pm_Y = numpyro.sample(\"pm_Y\", dist.Normal(mu, sigma), obs=Y)\n\n    return pm_Y\n\n\nMCMCã®å®Ÿè¡Œ\n\n# dfã‹ã‚‰numpy.ndarrayã«å¤‰æ›\nY = df[\"tip\"].values  # ç›®çš„å¤‰æ•°ï¼šY = [y0, y1, y2, ... yi]\nX = df[\"total_bill\"].values  # èª¬æ˜å¤‰æ•°ï¼šX = [x0, x1, x2, ... xi]\n\n\n# RUN MCMC\n# NUTS (No-U-Turn Sampler) is an adaptive variant of the Hamiltonian Monte Carlo (HMC) algorithm\n# HMCã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚„åå¾©å›æ•°ãªã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•èª¿æ•´\n# åŠ¹ç‡çš„ãªã‚µãƒ³ãƒ—ãƒ«\n# åæŸã®æ”¹å–„\n# https://qiita.com/dai08srhg/items/5d4ac3070bae836aef10\nkernel = NUTS(model)\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=2000)\nmcmc.run(rng_key=random.PRNGKey(0), X=X, Y=Y)\n\n# print MCMC summaryï¼ˆæ¨å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆé‡ï¼‰\nmcmc.print_summary()\n\nsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:00&lt;00:00, 3616.90it/s, 11 steps of size 2.54e-01. acc. prob=0.94]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n      pm_a      0.11      0.01      0.11      0.09      0.12    368.76      1.00\n      pm_b      0.90      0.16      0.90      0.65      1.17    377.02      1.00\n\nNumber of divergences: 0\n\n\n\n\n\n\n# get param (num_samples=2000)\nmcmc_samples = mcmc.get_samples()\npm_a = mcmc_samples[\"pm_a\"]\npm_b = mcmc_samples[\"pm_b\"]\n\nprint(\"mcmc_samples:\", mcmc_samples)\nprint(\"pm_a:\", pm_a)\nprint(\"pm_b:\", pm_b)\n\nmcmc_samples: {'pm_a': Array([0.10649762, 0.110033  , 0.10530762, ..., 0.11129791, 0.10644959,\n       0.10579225], dtype=float32), 'pm_b': Array([0.86267745, 0.9398046 , 0.9131396 , ..., 0.751051  , 0.8998211 ,\n       0.8538504 ], dtype=float32)}\npm_a: [0.10649762 0.110033   0.10530762 ... 0.11129791 0.10644959 0.10579225]\npm_b: [0.86267745 0.9398046  0.9131396  ... 0.751051   0.8998211  0.8538504 ]\n\n\n\nlen(set(pm_a.tolist()))\n\n1971\n\n\n\n# å¯è¦–åŒ–\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.distplot(pm_a)\nplt.title(\"Probability Density of pm_a\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(pm_b)\nplt.title(\"Probability Density of pm_b\")\nplt.show()\n\nC:\\Users\\takuk\\AppData\\Local\\Temp\\ipykernel_27764\\3318693772.py:5: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(pm_a)\nC:\\Users\\takuk\\AppData\\Local\\Temp\\ipykernel_27764\\3318693772.py:9: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(pm_b)\n\n\n\n\n\n\n\n\n\n\nX_range = jnp.linspace(0, 50, 51)\n\npredictive = Predictive(model, mcmc_samples)\npredict_samples = predictive(random.PRNGKey(1), X=X_range, Y=None)\nprint(predict_samples)\n\n{'pm_Y': Array([[ 1.810429  ,  0.02438818,  0.22200845, ...,  5.2724934 ,\n         6.715812  ,  6.422859  ],\n       [ 0.15076251,  0.8095171 ,  0.21540537, ...,  5.2437186 ,\n         7.817451  ,  6.173345  ],\n       [ 2.476837  ,  0.21040367,  0.42815638, ...,  8.04783   ,\n         7.40407   ,  5.8891115 ],\n       ...,\n       [ 1.0230944 ,  1.1552314 ,  0.25326768, ...,  4.831621  ,\n         7.0255423 ,  5.759563  ],\n       [ 1.5060548 , -0.62601656,  1.4508446 , ...,  5.9778643 ,\n         6.058488  ,  4.774219  ],\n       [ 1.0496006 ,  0.51519567,  1.4674567 , ...,  5.349556  ,\n         4.992259  ,  6.651703  ]], dtype=float32)}\n\n\n\npm_Y = predict_samples[\"pm_Y\"]\nprint(pm_Y)\n\n[[ 1.810429    0.02438818  0.22200845 ...  5.2724934   6.715812\n   6.422859  ]\n [ 0.15076251  0.8095171   0.21540537 ...  5.2437186   7.817451\n   6.173345  ]\n [ 2.476837    0.21040367  0.42815638 ...  8.04783     7.40407\n   5.8891115 ]\n ...\n [ 1.0230944   1.1552314   0.25326768 ...  4.831621    7.0255423\n   5.759563  ]\n [ 1.5060548  -0.62601656  1.4508446  ...  5.9778643   6.058488\n   4.774219  ]\n [ 1.0496006   0.51519567  1.4674567  ...  5.349556    4.992259\n   6.651703  ]]\n\n\n\nlen(pm_Y)\n\n2000\n\n\n\n\næ¨è«–\n\nmean_Y = pm_Y.mean(axis=0)\ny_low, y_high = jnp.percentile(pm_Y, jnp.array([2.5, 97.5]), axis=0)\n\n# å¯è¦–åŒ–\nfig = plt.figure(figsize=(6.0, 6.0))\nplt.plot(X_range, mean_Y, \"-\", color=\"g\")\nplt.fill_between(X_range, y_low, y_high, color=\"g\", alpha=0.3)\nplt.plot(X, Y, \"o\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nå…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ã‚¿ãƒ¼ã‚’ä½¿ã£ã¦æ¨è«–\n\nN = len(pm_a)\n# X_range = jnp.linspace(0, 50, 50)\n\n# äºˆæ¸¬çµæœã®å¯è¦–åŒ–\nfig = plt.figure(figsize=(6.0, 6.0))\nfor i in range(N):\n    pm_y = [pm_a[i] * x + pm_b[i] for x in X_range]\n    plt.plot(X_range, pm_y, \"g-\", alpha=0.5)\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¯è¦–åŒ–\nplt.plot(X, Y, \"o\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.title(\"linear regression\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndef my_model(mcmc_samples, X):\n    \"\"\"ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«\"\"\"\n    mcmc_samples = mcmc.get_samples()\n    pm_a = mcmc_samples[\"pm_a\"]\n    pm_b = mcmc_samples[\"pm_b\"]\n\n    # å¹³å‡å€¤\n    mu_a = jnp.mean(pm_a, axis=0)\n    mu_b = jnp.mean(pm_b, axis=0)\n    print(\"mu_a=\", mu_a)\n    print(\"mu_b=\", mu_b)\n\n    # ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«\n    Y_hat = mu_a * X + mu_b\n\n    return Y_hat\n\n\n# äºˆæ¸¬å€¤\nY_hat = my_model(mcmc_samples, X)\n\n# å¯è¦–åŒ–\nfig = plt.figure(figsize=(6.0, 6.0))\nplt.plot(X, Y, \"o\")\nplt.plot(X, Y_hat, \"g-\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.title(\"linear regression\")\nplt.show()\n\nmu_a= 0.10581156\nmu_b= 0.9027909"
  },
  {
    "objectID": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html#ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ¢ãƒ‡ãƒ«",
    "href": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html#ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ¢ãƒ‡ãƒ«",
    "title": "NumPyroã‚’ä½¿ã£ã¦ãƒ™ã‚¤ã‚ºå›å¸°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…",
    "section": "ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ¢ãƒ‡ãƒ«",
    "text": "ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ¢ãƒ‡ãƒ«\n\n\n\nimage.png\n\n\n\n# ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆ\ndef model(X, Y=None):\n    pm_a = numpyro.sample(\"pm_a\", dist.Normal(0.0, 10.0))\n    pm_b = numpyro.sample(\"pm_b\", dist.Normal(0.0, 10.0))\n\n    theta = pm_a * X + pm_b\n    mu = jnp.exp(theta)\n\n    numpyro.sample(\"pm_Y\", dist.Poisson(mu), obs=Y)\n\n\n# Run MCMC\nkernel = NUTS(model)\n# mcmc = MCMC(kernel, num_warmup=1000, num_samples=2000, num_chains=4)\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=2000)\nmcmc.run(rng_key=random.PRNGKey(0), X=X, Y=Y)\n\n# print MCMC summaryï¼ˆäº‹å¾Œåˆ†å¸ƒã®åŸºæœ¬çµ±è¨ˆé‡ã‚’ç¢ºèªï¼‰\nmcmc.print_summary()\n\nsample: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3000/3000 [00:00&lt;00:00, 3858.36it/s, 15 steps of size 2.37e-01. acc. prob=0.94]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n      pm_a      0.03      0.00      0.03      0.02      0.04    435.78      1.00\n      pm_b      0.46      0.09      0.45      0.31      0.59    399.54      1.00\n\nNumber of divergences: 0\n\n\n\n\n\n\n# get param (num_samples=2000)\nmcmc_samples = mcmc.get_samples()\npm_a = mcmc_samples[\"pm_a\"]\npm_b = mcmc_samples[\"pm_b\"]\n\nprint(\"mcmc_samples:\", mcmc_samples)\nprint(\"pm_a:\", pm_a)\nprint(\"pm_b:\", pm_b)\n\nmcmc_samples: {'pm_a': Array([0.03003247, 0.0300827 , 0.02773185, ..., 0.03265581, 0.03095045,\n       0.03059688], dtype=float32), 'pm_b': Array([0.5058282 , 0.5151023 , 0.5058284 , ..., 0.37341517, 0.4465436 ,\n       0.42378387], dtype=float32)}\npm_a: [0.03003247 0.0300827  0.02773185 ... 0.03265581 0.03095045 0.03059688]\npm_b: [0.5058282  0.5151023  0.5058284  ... 0.37341517 0.4465436  0.42378387]\n\n\n\n# å¯è¦–åŒ–\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.distplot(pm_a)\nplt.title(\"Probability Density of pm_a\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(pm_b)\nplt.title(\"Probability Density of pm_b\")\nplt.show()\n\nC:\\Users\\takuk\\AppData\\Local\\Temp\\ipykernel_27764\\3318693772.py:5: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(pm_a)\nC:\\Users\\takuk\\AppData\\Local\\Temp\\ipykernel_27764\\3318693772.py:9: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(pm_b)\n\n\n\n\n\n\n\n\n\n\næ¨è«–\n\nX_range = jnp.linspace(0, 50, 50)\n\npredictive = Predictive(model, mcmc_samples)\npredict_samples = predictive(random.PRNGKey(0), X=X_range, Y=None)\n# print(predict_samples)\n\npm_Y = predict_samples[\"pm_Y\"]\n# print(pm_Y)\n\nmean_Y = pm_Y.mean(axis=0)\ny_low, y_high = jnp.percentile(pm_Y, jnp.array([2.5, 97.5]), axis=0)\n\n# å¯è¦–åŒ–\nfig = plt.figure(figsize=(6.0, 6.0))\nplt.plot(X_range, mean_Y, \"-\", color=\"g\")\nplt.fill_between(X_range, y_low, y_high, color=\"g\", alpha=0.3)\nplt.plot(X, Y, \"o\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nå…¨ã¦ã®æ¨å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦æ¨è«–\n\nN = len(pm_a)\n# X_range = jnp.linspace(0, 50, 50)\n\n# äºˆæ¸¬çµæœã®å¯è¦–åŒ–\nfig = plt.figure(figsize=(6.0, 6.0))\nfor i in range(N):\n    pm_y = [jnp.exp(pm_a[i] * x + pm_b[i]) for x in X_range]\n    plt.plot(X_range, pm_y, \"g-\", alpha=0.5)\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¯è¦–åŒ–\nplt.plot(X, Y, \"o\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.title(\"linear regression\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndef my_model(mcmc_samples, X):\n    \"\"\"ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ¢ãƒ‡ãƒ«\"\"\"\n    mcmc_samples = mcmc.get_samples()\n    pm_a = mcmc_samples[\"pm_a\"]\n    pm_b = mcmc_samples[\"pm_b\"]\n\n    # å¹³å‡å€¤\n    mu_a = jnp.mean(pm_a, axis=0)\n    mu_b = jnp.mean(pm_b, axis=0)\n    print(\"mu_a=\", mu_a)\n    print(\"mu_b=\", mu_b)\n\n    # ãƒã‚¢ã‚½ãƒ³å›å¸°ãƒ¢ãƒ‡ãƒ«\n    Y_hat = jnp.exp(mu_a * X + mu_b)\n\n    return Y_hat\n\n\n# äºˆæ¸¬å€¤\nY_hat = my_model(mcmc_samples, X_range)\n\n# å¯è¦–åŒ–\nfig = plt.figure(figsize=(6.0, 6.0))\nplt.plot(X, Y, \"o\")\nplt.plot(X_range, Y_hat, \"g-\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.title(\"poisson regression\")\nplt.show()\n\nmu_a= 0.030312581\nmu_b= 0.45623016"
  },
  {
    "objectID": "work/improve_python/demo_batch.html",
    "href": "work/improve_python/demo_batch.html",
    "title": "Pythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ¼ã§ãƒãƒƒãƒå‡¦ç†ã‚’è¡Œã†æ–¹æ³•",
    "section": "",
    "text": "import pandas as pd\nimport duckdb\nimport faker\n\nfrom itertools import batched\n\n\ndata = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n\n# 3å€‹ãšã¤ã®ãƒãƒƒãƒã«åˆ†å‰²\nfor batch in batched(data, 3):\n    print(batch)\n\n# å‡ºåŠ›:\n# ('A', 'B', 'C')\n# ('D', 'E', 'F')\n# ('G',)  &lt;- ä½™ã£ãŸåˆ†ã‚‚æœ€å¾Œã«å‡ºåŠ›ã•ã‚Œã‚‹\n\n('A', 'B', 'C')\n('D', 'E', 'F')\n('G',)\n\n\n\n# fakerã‚’ä½¿ã£ã¦100ä¸‡è¡Œã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã€DuckDBã«æŒ¿å…¥\nfake = faker.Faker()\n\n\n%%time\n\ndata = [{\n    \"name\":fake.name(),\n    \"sex\":fake.random_element(elements=[\"M\", \"F\"]),\n    \"age\":fake.random_int(min=18, max=80),\n    \"address\":fake.address().replace(\"\\n\", \", \"),\n    \"phone_number\":fake.phone_number(),\n} for i in range(1_000_000)\n]\n\ndf = pd.DataFrame(data)\n# display(df.head())\n# df.shape\n\nCPU times: total: 2min 2s\nWall time: 2min 2s\n\n\n\n%%time\n\n# ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿é–¢æ•°ã§ç„¡é™ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãŠã\ndef fake_data_generator():\n    while True:\n        yield {\n            \"name\":fake.name(),\n            \"sex\":fake.random_element(elements=[\"M\", \"F\"]),\n            \"age\":fake.random_int(min=18, max=80),\n            \"address\":fake.address().replace(\"\\n\", \", \"),\n            \"phone_number\":fake.phone_number(),\n        }\n\nall_dfs = []\nfor batch in batched(fake_data_generator(), 10000): # 1ä¸‡ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†\n    batch_df = pd.DataFrame(batch)\n    all_dfs.append(batch_df)\n    \n    if len(all_dfs) &gt;= 100: # 10000 * 100 = 1_000_000ä»¶ã§ã‚¹ãƒˆãƒƒãƒ—\n        break\n\ndf = pd.concat(all_dfs)\n\nCPU times: total: 2min 6s\nWall time: 2min 6s\n\n\n\noutput_csv = \"fake_data_100k.csv\"\ndf.to_csv(output_csv, index=False)\n\n\n%%timeit\nreader = pd.read_csv(output_csv, chunksize=10000)\n\ndf_tmps = []\nfor df_chunk in reader:\n    df_tmps.append(df_chunk)\ndf_tmp = pd.concat(df_tmps)\n\n# df_tmp.head()\n\n108 ms Â± 737 Î¼s per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n\n\n\n%%timeit\ndf_tmp = pd.read_csv(output_csv)\n\n82.1 ms Â± 817 Î¼s per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n\n\n\nimport time\n\nuser_ids = range(1000)\nfor batch in batched(user_ids, 50, strict=False):\n    print(batch)\n    time.sleep(1) # è² è·è»½æ¸›ã®ãŸã‚ã®å¾…æ©Ÿ\n\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49)\n(50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99)\n(100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149)\n(150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199)\n(200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249)\n(250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299)\n(300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349)\n(350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399)\n(400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449)\n(450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499)\n(500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549)\n(550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599)\n(600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649)\n(650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699)\n(700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749)\n(750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799)\n(800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849)\n(850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899)\n(900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949)\n(950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tako Data Science Lab",
    "section": "",
    "text": "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å†™çœŸ\n\n\nåå‰: æ± ç”° æ‹“å“‰ï¼ˆTakoï¼‰\næ‰€å±: æ ªå¼ä¼šç¤¾PREVENT\nè·ç¨®: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ\n\n\n\nã€ä¸ç™»æ ¡ãƒ»é€€å­¦ã€ãã—ã¦æµ·å¤–ã¸ã€‘ ç¥æˆ¸å­¦é™¢å¤§å­¦çµŒæ¸ˆå­¦éƒ¨ã«åœ¨å­¦ä¸­ã€å°†æ¥ã¸ã®æ¼ ç„¶ã¨ã—ãŸä¸å®‰ã‹ã‚‰ä¸ç™»æ ¡ã«ãªã‚Šä¸­é€€ã€‚ãã®å¾Œã€å¿ƒæ©Ÿä¸€è»¢ã—ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«ã¨ãƒãƒ¬ãƒ¼ã‚·ã‚¢ã¸ä¸€äººæ—…ã«å‡ºã¾ã—ãŸã€‚ãã“ã§å‡ºä¼šã£ãŸäººãŸã¡ã¨ã®äº¤æµã‚’é€šã˜ã¦ã€Œè‹±èªã‚’å­¦ã³ãŸã„ã€ã¨å¼·ãæ€ã„ã€ã‚¢ãƒ¡ãƒªã‚«ã®ãƒšãƒ³ã‚·ãƒ«ãƒ™ãƒ‹ã‚¢å·ã«ã‚ã‚‹ Delaware County Community College (DCCC) ã¸ã®ç•™å­¦ã‚’æ±ºæ„ã—ã¾ã—ãŸã€‚\nã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨çµ±è¨ˆå­¦ã¨ã®å‡ºä¼šã„ã€‘ ç•™å­¦å½“åˆã¯ãƒ“ã‚¸ãƒã‚¹å°‚æ”»ã§ã—ãŸãŒã€ãƒ«ãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆã®å½±éŸ¿ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«èˆˆå‘³ã‚’æŒã¡ã€Computer Scienceå°‚æ”»ã«å¤‰æ›´ã€‚ã“ã“ã§C++ã¨çµ±è¨ˆå­¦ã«å‡ºä¼šã£ãŸã“ã¨ãŒã€ç¾åœ¨ã®ã‚­ãƒ£ãƒªã‚¢ã®åŸç‚¹ã§ã™ã€‚2019å¹´ã«åŒæ ¡ã‚’å’æ¥­å¾Œã€å¸°å›½ã€‚\nã€ã‚­ãƒ£ãƒªã‚¢ã®è»¢æ›ç‚¹ã€‘ å¸°å›½å¾Œã¯ãƒ•ãƒªãƒ¼ã‚¿ãƒ¼ã¨ã—ã¦æ´»å‹•ã—ã¦ã„ã¾ã—ãŸãŒã€2020å¹´3æœˆã«æ ªå¼ä¼šç¤¾ãƒªã‚¯ãƒ«ãƒ¼ãƒˆR&Dã«å…¥ç¤¾ã€‚å®Ÿå‹™ã®ä¸­ã§ã€Œè‡ªåˆ†ã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã«ãªã‚ŠãŸã„ã€ã¨ã„ã†ç›®æ¨™ã‚’è¦‹å‡ºã—ã€è»¢è·ã‚’æ±ºæ„ã—ã¾ã—ãŸã€‚\nã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã¨ã—ã¦ã®æ­©ã¿ã€‘ 2021å¹´11æœˆã€ã‚³ã‚°ãƒ©ãƒ•æ ªå¼ä¼šç¤¾ã«å…¥ç¤¾ã€‚SQLã€Pythonã€BIãƒ„ãƒ¼ãƒ«ã‚’ç”¨ã„ãŸåˆ†æå®Ÿå‹™ã®ã»ã‹ã€ETLæ§‹ç¯‰ã‚„ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼ˆéƒ¨é•·è·ï¼‰ã‚’çµŒé¨“ã—ã¾ã—ãŸã€‚ ãã®å¾Œã€9ãƒ¶æœˆé–“ã®ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹æœŸé–“ã‚’çµŒã¦ã€ã‚ˆã‚Šäº‹æ¥­ã®æˆé•·ã¨è²¬ä»»ã«ã‚³ãƒŸãƒƒãƒˆã™ã‚‹ãŸã‚ã€2025å¹´2æœˆã« æ ªå¼ä¼šç¤¾PREVENT ã«å…¥ç¤¾ã€‚ç¾åœ¨ã¯åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿åˆ†æã«å¾“äº‹ã—ã¦ã„ã¾ã™ã€‚\n\n\n\n\nğŸ“Š ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª\nğŸ“ æŠ€è¡“ãƒ–ãƒ­ã‚° (Zenn)\n\n\n\n\n\nEmail: ikedat350@gmail.com\nSNS: X (Twitter) / Wantedly"
  },
  {
    "objectID": "index.html#ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«",
    "href": "index.html#ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«",
    "title": "Tako Data Science Lab",
    "section": "",
    "text": "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å†™çœŸ\n\n\nåå‰: æ± ç”° æ‹“å“‰ï¼ˆTakoï¼‰\næ‰€å±: æ ªå¼ä¼šç¤¾PREVENT\nè·ç¨®: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ\n\n\n\nã€ä¸ç™»æ ¡ãƒ»é€€å­¦ã€ãã—ã¦æµ·å¤–ã¸ã€‘ ç¥æˆ¸å­¦é™¢å¤§å­¦çµŒæ¸ˆå­¦éƒ¨ã«åœ¨å­¦ä¸­ã€å°†æ¥ã¸ã®æ¼ ç„¶ã¨ã—ãŸä¸å®‰ã‹ã‚‰ä¸ç™»æ ¡ã«ãªã‚Šä¸­é€€ã€‚ãã®å¾Œã€å¿ƒæ©Ÿä¸€è»¢ã—ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«ã¨ãƒãƒ¬ãƒ¼ã‚·ã‚¢ã¸ä¸€äººæ—…ã«å‡ºã¾ã—ãŸã€‚ãã“ã§å‡ºä¼šã£ãŸäººãŸã¡ã¨ã®äº¤æµã‚’é€šã˜ã¦ã€Œè‹±èªã‚’å­¦ã³ãŸã„ã€ã¨å¼·ãæ€ã„ã€ã‚¢ãƒ¡ãƒªã‚«ã®ãƒšãƒ³ã‚·ãƒ«ãƒ™ãƒ‹ã‚¢å·ã«ã‚ã‚‹ Delaware County Community College (DCCC) ã¸ã®ç•™å­¦ã‚’æ±ºæ„ã—ã¾ã—ãŸã€‚\nã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨çµ±è¨ˆå­¦ã¨ã®å‡ºä¼šã„ã€‘ ç•™å­¦å½“åˆã¯ãƒ“ã‚¸ãƒã‚¹å°‚æ”»ã§ã—ãŸãŒã€ãƒ«ãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆã®å½±éŸ¿ã§ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«èˆˆå‘³ã‚’æŒã¡ã€Computer Scienceå°‚æ”»ã«å¤‰æ›´ã€‚ã“ã“ã§C++ã¨çµ±è¨ˆå­¦ã«å‡ºä¼šã£ãŸã“ã¨ãŒã€ç¾åœ¨ã®ã‚­ãƒ£ãƒªã‚¢ã®åŸç‚¹ã§ã™ã€‚2019å¹´ã«åŒæ ¡ã‚’å’æ¥­å¾Œã€å¸°å›½ã€‚\nã€ã‚­ãƒ£ãƒªã‚¢ã®è»¢æ›ç‚¹ã€‘ å¸°å›½å¾Œã¯ãƒ•ãƒªãƒ¼ã‚¿ãƒ¼ã¨ã—ã¦æ´»å‹•ã—ã¦ã„ã¾ã—ãŸãŒã€2020å¹´3æœˆã«æ ªå¼ä¼šç¤¾ãƒªã‚¯ãƒ«ãƒ¼ãƒˆR&Dã«å…¥ç¤¾ã€‚å®Ÿå‹™ã®ä¸­ã§ã€Œè‡ªåˆ†ã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ¼ã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã«ãªã‚ŠãŸã„ã€ã¨ã„ã†ç›®æ¨™ã‚’è¦‹å‡ºã—ã€è»¢è·ã‚’æ±ºæ„ã—ã¾ã—ãŸã€‚\nã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã¨ã—ã¦ã®æ­©ã¿ã€‘ 2021å¹´11æœˆã€ã‚³ã‚°ãƒ©ãƒ•æ ªå¼ä¼šç¤¾ã«å…¥ç¤¾ã€‚SQLã€Pythonã€BIãƒ„ãƒ¼ãƒ«ã‚’ç”¨ã„ãŸåˆ†æå®Ÿå‹™ã®ã»ã‹ã€ETLæ§‹ç¯‰ã‚„ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆï¼ˆéƒ¨é•·è·ï¼‰ã‚’çµŒé¨“ã—ã¾ã—ãŸã€‚ ãã®å¾Œã€9ãƒ¶æœˆé–“ã®ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹æœŸé–“ã‚’çµŒã¦ã€ã‚ˆã‚Šäº‹æ¥­ã®æˆé•·ã¨è²¬ä»»ã«ã‚³ãƒŸãƒƒãƒˆã™ã‚‹ãŸã‚ã€2025å¹´2æœˆã« æ ªå¼ä¼šç¤¾PREVENT ã«å…¥ç¤¾ã€‚ç¾åœ¨ã¯åŒ»ç™‚ãƒ‡ãƒ¼ã‚¿åˆ†æã«å¾“äº‹ã—ã¦ã„ã¾ã™ã€‚\n\n\n\n\nğŸ“Š ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª\nğŸ“ æŠ€è¡“ãƒ–ãƒ­ã‚° (Zenn)\n\n\n\n\n\nEmail: ikedat350@gmail.com\nSNS: X (Twitter) / Wantedly"
  },
  {
    "objectID": "demo.html",
    "href": "demo.html",
    "title": "Lab",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\nNumPyroã‚’ä½¿ã£ã¦ãƒ™ã‚¤ã‚ºå›å¸°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…\n\n\n\nStatistics\n\nBayesian\n\nDemo\n\n\n\nãƒ™ã‚¤ã‚ºå›å¸°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã‚’NumPyroã‚’ä½¿ã£ã¦è¡Œã†\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\nSARIMAã‚’ä½¿ã£ãŸåŒ»ç™‚è²»äºˆæ¸¬ã®ãƒ‡ãƒ¢\n\n\n\nStatistics\n\nTime Series\n\nDemo\n\n\n\nSARIMAï¼ˆSeasonal AutoRegressive Integrated Moving Averageï¼‰ã¯ã€å­£ç¯€æ€§ã®ã‚ã‚‹æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚åŒ»ç™‚è²»ã®éšå·®ã‚’å–ã‚‹ã¨ã€å®šå¸¸åˆ†å¸ƒã«è¿‘ã¥ãã€åŒ»ç™‚è²»ã¯å†¬ã¯ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚¶ãŒæµè¡Œã—ãŸã‚Šå¤ã¯ç†±ä¸­ç—‡ãŒå¢—ãˆãŸã‚Šã¨å­£ç¯€æ€§ãŒã‚ã‚‹ãŸã‚ã€SARIMAãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦åŒ»ç™‚è²»ã®äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\nåŸºæœ¬çš„ãªå› å­åˆ†æã®ãƒ‡ãƒ¢\n\n\n\nStatistics\n\nFactor Analysis\n\nDemo\n\n\n\nå› å­åˆ†æã®åŸºæœ¬çš„ãªå®Ÿè£…ã‚’factor_analyzerã‚’ä½¿ã£ã¦è¡Œã†\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\nPythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ¼ã§ãƒãƒƒãƒå‡¦ç†ã‚’è¡Œã†æ–¹æ³•\n\n\n\nPython Fundamentals\n\nDemo\n\nPython General Library\n\n\n\nitertools.batched ã¯ã€Python 3.12 ã§å°å…¥ã•ã‚ŒãŸã€Œãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã—ãŸã‚µã‚¤ã‚ºã”ã¨ã«åˆ†å‰²ã—ã¦ï¼ˆãƒãƒƒãƒåŒ–ã—ã¦ï¼‰å–ã‚Šå‡ºã™ã€ãŸã‚ã®éå¸¸ã«ä¾¿åˆ©ãªé–¢æ•°ã§ã™ã€‚ã“ã‚Œã¾ã§ã€æ©Ÿæ¢°å­¦ç¿’ã®ãƒŸãƒ‹ãƒãƒƒãƒä½œæˆã‚„ã€APIã¸ã®å°åˆ†ã‘é€ä¿¡ã€å·¨å¤§ãªãƒ­ã‚°ã®åˆ†å‰²å‡¦ç†ãªã©ã‚’è¡Œã†éš›ã¯ã€ãƒˆãƒªãƒƒã‚­ãƒ¼ãªè‡ªä½œé–¢æ•°ã‚„ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£è£½ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆmore-itertools ãªã©ï¼‰ã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸãŒã€ãã‚ŒãŒæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã ã‘ã§ã‚¹ãƒãƒ¼ãƒˆã«æ›¸ã‘ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n\n\n\n\n\nJan 31, 2026\n\n\n\n\n\n\n\nT-stringã®å®Ÿæ¼”\n\n\n\nPython Fundamentals\n\nDemo\n\nPython General Library\n\n\n\nã“ã®æ©Ÿèƒ½ã¯ã€å¾“æ¥ã® f-strings ã¨è¦‹ãŸç›®ã¯ä¼¼ã¦ã„ã¾ã™ãŒã€ã€Œæ–‡å­—åˆ—ã‚’ã™ãã«ä½œã‚‰ãšã€æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿æŒã™ã‚‹ã€ã¨ã„ã†ç‚¹ãŒæ±ºå®šçš„ã«é•ã„ã¾ã™ã€‚ã¾ã uvã®ç’°å¢ƒã§ã¯ã§ããªã„ã€‚ã€‚\n\n\n\n\n\nJan 31, 2026\n\n\n\n\n\n\n\nSteamã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸEDA\n\n\n\nAnalysis\n\nSteam\n\n\n\nSteamã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸEDAã‚’å®Ÿæ–½ã—ã€å£²ã‚Œç­‹ã®ã‚«ãƒ†ã‚´ãƒªãªã©ã‚’åˆ†æã™ã‚‹\n\n\n\n\n\nFeb 1, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "work/analysis/steam-analysis/script/steam_data_extraction.html",
    "href": "work/analysis/steam-analysis/script/steam_data_extraction.html",
    "title": "Steamã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸEDA",
    "section": "",
    "text": "import os\nimport requests\nimport pandas as pd\n\n\n# api_key = os.environ.get(\"API_KEY\")\napi_key = \"__\"\n\nprint(api_key)\n\n\ndef GetOwnedGames(api_key, uid):\n    url = \"http://api.steampowered.com/IPlayerService/GetOwnedGames/v1/?key={}&steamid={}&format=json\".format(\n        api_key, uid\n    )\n    r = requests.get(url)\n    data = json.loads(r.text)\n    return data[\"response\"]"
  },
  {
    "objectID": "work/improve_python/demo_tstring.html",
    "href": "work/improve_python/demo_tstring.html",
    "title": "T-stringã®å®Ÿæ¼”",
    "section": "",
    "text": "# Templateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\nfrom string.templatelib import Template\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 2\n      1 # Templateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n----&gt; 2 from string.templatelib import Template\n\nModuleNotFoundError: No module named 'string.templatelib'; 'string' is not a package\n\n\n\n\nname = \"Python\"\n# f-string: ã™ãã« \"Hello Python\" ã¨ã„ã†æ–‡å­—åˆ—ã«ãªã‚‹\nf_res = f\"Hello {name}\" \n\n# t-string: æ§‹é€ ã‚’ä¿æŒã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ãªã‚‹\nt_res = t\"Hello {name}\"\n\nprint(type(t_res)) # &lt;class 'string.templatelib.Template'&gt;\nprint(t_res.strings) # ('Hello ', '')\nprint(t_res.values)  # ('Python',)"
  },
  {
    "objectID": "work/statistic_methods/basic_factor_analyze/factor_analyze_demo.html",
    "href": "work/statistic_methods/basic_factor_analyze/factor_analyze_demo.html",
    "title": "åŸºæœ¬çš„ãªå› å­åˆ†æã®ãƒ‡ãƒ¢",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom factor_analyzer import FactorAnalyzer\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nfrom factor_analyzer.factor_analyzer import calculate_kmo\n\n# ã‚°ãƒ©ãƒ•ã®ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š\nsns.set(style='whitegrid')\n# æ³¨æ„: æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã¯ç’°å¢ƒã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼ˆColabã®å ´åˆã¯åˆ¥é€”ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå¿…è¦ï¼‰\n# ã“ã“ã§ã¯è‹±èªãƒ©ãƒ™ãƒ«ã§é€²è¡Œã—ã¾ã™ãŒã€æ„å‘³ã¯è§£èª¬ã—ã¾ã™ã€‚\n\n\nãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ\nå› å­åˆ†æãŒç¶ºéº—ã«æ±ºã¾ã‚‹ã‚ˆã†ã€æ„å›³çš„ã«ç›¸é–¢ã‚’æŒãŸã›ãŸå¥è¨ºãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã™ã€‚\nã“ã“ã§ã¯2ã¤ã®æ½œåœ¨å› å­ï¼ˆ1. ä»£è¬ãƒ»é‹å‹•ä¸è¶³å› å­ã€2. ã‚¹ãƒˆãƒ¬ã‚¹ãƒ»è¡€åœ§å› å­ï¼‰ã‚’æƒ³å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n\n# ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®å›ºå®š\nnp.random.seed(42)\nn_samples = 500\n\n# æ½œåœ¨å¤‰æ•°ï¼ˆçœŸã®è¦å› ï¼‰ã‚’ä½œæˆ\n# Factor 1: é‹å‹•ä¸è¶³ãƒ»è‚¥æº€å‚¾å‘ (Metabolic)\nf_metabolic = np.random.normal(0, 1, n_samples)\n# Factor 2: ã‚¹ãƒˆãƒ¬ã‚¹ãƒ»äº¤æ„Ÿç¥çµŒç·Šå¼µ (Stress)\nf_stress = np.random.normal(0, 1, n_samples)\n\n# è¦³æ¸¬å¤‰æ•°ï¼ˆå¥è¨ºé …ç›®ï¼‰ã‚’ä½œæˆ\n# å¼: è¦³æ¸¬å€¤ = è² è·é‡ * å› å­ + ãƒã‚¤ã‚º\ndata = {\n    # --- å› å­1ï¼ˆä»£è¬ãƒ»è‚¥æº€ï¼‰ã®å½±éŸ¿ãŒå¼·ã„é …ç›® ---\n    'BMI': 0.8 * f_metabolic + 0.1 * f_stress + np.random.normal(0, 0.5, n_samples),\n    'è…¹å›²(Waist)': 0.85 * f_metabolic + 0.1 * f_stress + np.random.normal(0, 0.5, n_samples),\n    'ä¸­æ€§è„‚è‚ª(TG)': 0.7 * f_metabolic + 0.2 * f_stress + np.random.normal(0, 0.6, n_samples),\n    'ç©ºè…¹æ™‚è¡€ç³–(FPG)': 0.6 * f_metabolic + 0.3 * f_stress + np.random.normal(0, 0.6, n_samples),\n    'HDLã‚³ãƒ¬ã‚¹ãƒ†ãƒ­ãƒ¼ãƒ«': -0.7 * f_metabolic + np.random.normal(0, 0.6, n_samples), # é€†ç›¸é–¢\n\n    # --- å› å­2ï¼ˆã‚¹ãƒˆãƒ¬ã‚¹ãƒ»è¡€åœ§ï¼‰ã®å½±éŸ¿ãŒå¼·ã„é …ç›® ---\n    'åç¸®æœŸè¡€åœ§(SBP)': 0.2 * f_metabolic + 0.85 * f_stress + np.random.normal(0, 0.5, n_samples),\n    'æ‹¡å¼µæœŸè¡€åœ§(DBP)': 0.2 * f_metabolic + 0.80 * f_stress + np.random.normal(0, 0.5, n_samples),\n    'è„ˆæ‹(Pulse)': 0.1 * f_metabolic + 0.6 * f_stress + np.random.normal(0, 0.7, n_samples),\n}\n\ndf = pd.DataFrame(data)\n\n# ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª\nprint(\"ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢è¡Œåˆ—ï¼ˆä¸€éƒ¨ï¼‰:\")\ndisplay(df.corr().round(2))\n\nãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢è¡Œåˆ—ï¼ˆä¸€éƒ¨ï¼‰:\n\n\n\n\n\n\n\n\n\nBMI\nè…¹å›²(Waist)\nä¸­æ€§è„‚è‚ª(TG)\nç©ºè…¹æ™‚è¡€ç³–(FPG)\nHDLã‚³ãƒ¬ã‚¹ãƒ†ãƒ­ãƒ¼ãƒ«\nåç¸®æœŸè¡€åœ§(SBP)\næ‹¡å¼µæœŸè¡€åœ§(DBP)\nè„ˆæ‹(Pulse)\n\n\n\n\nBMI\n1.00\n0.72\n0.62\n0.59\n-0.63\n0.21\n0.20\n0.16\n\n\nè…¹å›²(Waist)\n0.72\n1.00\n0.63\n0.60\n-0.64\n0.19\n0.18\n0.13\n\n\nä¸­æ€§è„‚è‚ª(TG)\n0.62\n0.63\n1.00\n0.55\n-0.52\n0.32\n0.30\n0.20\n\n\nç©ºè…¹æ™‚è¡€ç³–(FPG)\n0.59\n0.60\n0.55\n1.00\n-0.50\n0.40\n0.35\n0.28\n\n\nHDLã‚³ãƒ¬ã‚¹ãƒ†ãƒ­ãƒ¼ãƒ«\n-0.63\n-0.64\n-0.52\n-0.50\n1.00\n-0.11\n-0.07\n-0.05\n\n\nåç¸®æœŸè¡€åœ§(SBP)\n0.21\n0.19\n0.32\n0.40\n-0.11\n1.00\n0.74\n0.56\n\n\næ‹¡å¼µæœŸè¡€åœ§(DBP)\n0.20\n0.18\n0.30\n0.35\n-0.07\n0.74\n1.00\n0.53\n\n\nè„ˆæ‹(Pulse)\n0.16\n0.13\n0.20\n0.28\n-0.05\n0.56\n0.53\n1.00\n\n\n\n\n\n\n\n\n\nå› å­åˆ†æã®äº‹å‰æ¤œå®š\nãƒ‡ãƒ¼ã‚¿ãŒå› å­åˆ†æã«é©ã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ï¼ˆKMOæ¤œå®šã¨ãƒãƒ¼ãƒˆãƒ¬ãƒƒãƒˆæ¤œå®šï¼‰\n\n# 1. ãƒãƒ¼ãƒˆãƒ¬ãƒƒãƒˆã®çƒé¢æ€§æ¤œå®š\n# på€¤ãŒ0.05æœªæº€ãªã‚‰ã€ç›¸é–¢è¡Œåˆ—ã¯å˜ä½è¡Œåˆ—ã§ã¯ãªã„ï¼ˆå› å­åˆ†æã™ã‚‹æ„å‘³ãŒã‚ã‚‹ï¼‰\nchi_square_value, p_value = calculate_bartlett_sphericity(df)\nprint(f\"Bartlett's test p-value: {p_value:.3e}\")\n\n# 2. KMO (Kaiser-Meyer-Olkin) æ¤œå®š\n# 0.6ä»¥ä¸Šã§ã‚ã‚Œã°å› å­åˆ†æã«é©ã—ã¦ã„ã‚‹ã¨ã•ã‚Œã‚‹\nkmo_all, kmo_model = calculate_kmo(df)\nprint(f\"KMO Test Value: {kmo_model:.3f}\")\n\nif kmo_model &gt; 0.6 and p_value &lt; 0.05:\n    print(\"&gt;&gt; åˆ¤å®š: ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯å› å­åˆ†æã«é©ã—ã¦ã„ã¾ã™ã€‚\")\nelse:\n    print(\"&gt;&gt; åˆ¤å®š: ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ç›´ã™å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\")\n\nBartlett's test p-value: 0.000e+00\nKMO Test Value: 0.846\n&gt;&gt; åˆ¤å®š: ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯å› å­åˆ†æã«é©ã—ã¦ã„ã¾ã™ã€‚\n\n\n\n\nå› å­ã®æ•°ã‚’æ±ºã‚ã‚‹ï¼ˆã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆï¼‰\nå›ºæœ‰å€¤ã‚’è¨ˆç®—ã—ã€ã„ãã¤ã®å› å­ã‚’æŠ½å‡ºãƒ»ä¿æŒã™ã¹ãã‹ã‚’ã‚°ãƒ©ãƒ•ã§ç¢ºèªã—ã¾ã™ã€‚\n\n# å› å­æ•°ã‚’å¤‰æ•°ã®æ•°ã ã‘è¨­å®šã—ã¦ä»®å®Ÿè¡Œ\nfa = FactorAnalyzer(n_factors=len(df.columns), rotation=None)\nfa.fit(df)\n\n# å›ºæœ‰å€¤ï¼ˆEigenvaluesï¼‰ã®å–å¾—\nev, v = fa.get_eigenvalues()\n\n# ã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆã®æç”»\nplt.figure(figsize=(8, 5))\nplt.scatter(range(1, df.shape[1]+1), ev)\nplt.plot(range(1, df.shape[1]+1), ev)\nplt.title('Scree Plot')\nplt.xlabel('Number of Factors')\nplt.ylabel('Eigenvalue')\nplt.axhline(y=1, color='r', linestyle='--') # åŸºæº–ç·šï¼ˆå›ºæœ‰å€¤1ä»¥ä¸Šã‚’æ¡ç”¨ã™ã‚‹ã“ã¨ãŒå¤šã„ï¼‰\nplt.grid(True)\nplt.show()\n\nprint(\"å›ºæœ‰å€¤:\", ev.round(2))\n# é€šå¸¸ã€å›ºæœ‰å€¤ãŒæ€¥æ¿€ã«ä¸‹ãŒã£ã¦ãªã ã‚‰ã‹ã«ãªã‚‹ç›´å‰ã€ã¾ãŸã¯1ä»¥ä¸Šã®æ•°ã‚’é¸ã³ã¾ã™ã€‚\n# ä»Šå›ã®è¨­è¨ˆã§ã¯ã€Œ2ã€ã¾ãŸã¯ã€Œ3ã€ã‚ãŸã‚Šã§æŠ˜ã‚Œã‚‹ã¯ãšã§ã™ã€‚\n\n\n\n\n\n\n\n\nå›ºæœ‰å€¤: [3.83 1.89 0.53 0.44 0.43 0.35 0.28 0.25]\n\n\n\n\nå› å­åˆ†æã®å®Ÿè¡Œã¨ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å¯è¦–åŒ–\nå› å­æ•°ã‚’ã€Œ2ã€ã¨ä»®å®šã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚ å›è»¢æ³•ã«ã¯ promaxï¼ˆæ–œäº¤å›è»¢ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€å¥åº·è¦å› åŒå£«ï¼ˆè‚¥æº€ã¨ã‚¹ãƒˆãƒ¬ã‚¹ï¼‰ã«ã¯ç›¸é–¢ãŒã‚ã‚‹ã“ã¨ãŒè‡ªç„¶ã ã‹ã‚‰ã§ã™ã€‚\n\nimport plotly.express as px\n\n# --- åˆ†æãƒ‘ãƒ¼ãƒˆï¼ˆå‰å›ã¨åŒã˜ï¼‰---\n# å› å­ã®æ•°ã‚’æŒ‡å®šï¼ˆã‚¹ã‚¯ãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆã®çµæœã‚’è¦‹ã¦ 2 ã¨ä»®å®šï¼‰\nn_factors = 2\n\n# å› å­åˆ†æã®å®Ÿè¡Œ (promaxå›è»¢)\nfa = FactorAnalyzer(n_factors=n_factors, rotation='promax')\nfa.fit(df)\n\n# å› å­è² è·é‡ï¼ˆFactor Loadingsï¼‰ã®å–å¾—\nloadings = pd.DataFrame(fa.loadings_, \n                        index=df.columns, \n                        columns=[f'Factor{i+1}' for i in range(n_factors)])\n\n# --- å¯è¦–åŒ–ãƒ‘ãƒ¼ãƒˆ (Plotly) ---\nfig_heatmap = px.imshow(\n    loadings,\n    text_auto='.2f',  # å€¤ã‚’å°æ•°ç‚¹2æ¡ã§è¡¨ç¤º\n    aspect=\"auto\",\n    color_continuous_scale='RdBu_r', # èµ¤ã€œé’ï¼ˆç›¸é–¢ã®æ­£è² ã‚’è¡¨ç¾ï¼‰\n    zmin=-1, zmax=1,\n    title='Factor Loadings (å› å­è² è·é‡)'\n)\n\nfig_heatmap.update_layout(\n    xaxis_title=\"Factors\",\n    yaxis_title=\"Variables\",\n    width=600,\n    height=600\n)\n\nfig_heatmap.show()\n\n# å› å­ã®è§£é‡ˆãƒ’ãƒ³ãƒˆ\nprint(\"--- å› å­ã®è§£é‡ˆ ---\")\nprint(\"èµ¤è‰²ãŒå¼·ã„ç®‡æ‰€ = ãã®å› å­ã¨ã®æ­£ã®ç›¸é–¢ãŒå¼·ã„\")\nprint(\"é’è‰²ãŒå¼·ã„ç®‡æ‰€ = ãã®å› å­ã¨ã®è² ã®ç›¸é–¢ãŒå¼·ã„\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n--- å› å­ã®è§£é‡ˆ ---\nèµ¤è‰²ãŒå¼·ã„ç®‡æ‰€ = ãã®å› å­ã¨ã®æ­£ã®ç›¸é–¢ãŒå¼·ã„\né’è‰²ãŒå¼·ã„ç®‡æ‰€ = ãã®å› å­ã¨ã®è² ã®ç›¸é–¢ãŒå¼·ã„\n\n\n\n\nå› å­å¾—ç‚¹ã®ç®—å‡ºï¼ˆå€‹äººã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰\n\nimport plotly.graph_objects as go\n\n# --- è¨ˆç®—ãƒ‘ãƒ¼ãƒˆ ---\n# å› å­å¾—ç‚¹ã®è¨ˆç®—\nfactor_scores = pd.DataFrame(fa.transform(df), \n                            columns=[f'Factor{i+1}' for i in range(n_factors)])\n\n# å…ƒãƒ‡ãƒ¼ã‚¿ã¨çµåˆ\nresult_df = pd.concat([df, factor_scores], axis=1)\n\n# å› å­ã®å‘½åï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®çµæœã‚’è¦‹ã¦é©å®œå¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰\n# ã“ã“ã§ã¯ Factor1=ä»£è¬ãƒªã‚¹ã‚¯, Factor2=ã‚¹ãƒˆãƒ¬ã‚¹ãƒ»è¡€åœ§ãƒªã‚¹ã‚¯ ã¨ä»®å®š\nx_axis_label = 'Score_Metabolic(é‹å‹•ä¸è¶³)'\ny_axis_label = 'Score_Stress(é«˜è¡€åœ§è² è·)'\n\nresult_df = result_df.rename(columns={\n    'Factor1': x_axis_label,\n    'Factor2': y_axis_label\n})\n\n# --- å¯è¦–åŒ–ãƒ‘ãƒ¼ãƒˆ (Plotly) ---\nfig_scatter = px.scatter(\n    result_df, \n    x=x_axis_label, \n    y=y_axis_label,\n    # ãƒ›ãƒãƒ¼ã—ãŸæ™‚ã«å…ƒã®å¥è¨ºãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºï¼ˆåˆ†æã«ä¾¿åˆ©ï¼ï¼‰\n    hover_data=['BMI', 'åç¸®æœŸè¡€åœ§(SBP)', 'ç©ºè…¹æ™‚è¡€ç³–(FPG)'], \n    title='å—è¨ºè€…ã®å› å­ã‚¹ã‚³ã‚¢åˆ†å¸ƒï¼ˆãƒªã‚¹ã‚¯ãƒãƒƒãƒ—ï¼‰',\n    opacity=0.7\n)\n\n# ä¸­å¿ƒç·šï¼ˆ0,0ï¼‰ã‚’è¿½åŠ ã—ã¦è±¡é™ã‚’ã‚ã‹ã‚Šã‚„ã™ãã™ã‚‹\nfig_scatter.add_vline(x=0, line_width=1, line_dash=\"dash\", line_color=\"gray\")\nfig_scatter.add_hline(y=0, line_width=1, line_dash=\"dash\", line_color=\"gray\")\n\n# ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´\nfig_scatter.update_layout(\n    xaxis_title=x_axis_label,\n    yaxis_title=y_axis_label,\n    width=800,\n    height=600,\n    template='plotly_white'\n)\n\n# è±¡é™ã®æ³¨é‡ˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\nfig_scatter.add_annotation(x=2, y=2, text=\"é«˜ãƒªã‚¹ã‚¯ç¾¤\", showarrow=False, font=dict(color=\"red\"))\nfig_scatter.add_annotation(x=-2, y=-2, text=\"å¥åº·ç¾¤\", showarrow=False, font=dict(color=\"green\"))\n\nfig_scatter.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nãƒãƒ¼ãƒˆ\n\nå› å­è² è·é‡ (Loading): å¤‰æ•°ã¨å› å­ã®ç›¸é–¢ä¿‚æ•°ã®ã‚ˆã†ãªã‚‚ã®ã€‚çµ¶å¯¾å€¤ãŒ0.4ä»¥ä¸Šã‚ã‚Œã°é–¢ä¿‚ãŒå¼·ã„ã¨è¦‹ãªã—ã¾ã™ã€‚\nå›è»¢ (Rotation): promax ã¯æ–œäº¤å›è»¢ã§ã€å› å­é–“ã®ç›¸é–¢ã‚’è¨±å®¹ã—ã¾ã™ï¼ˆç¾å®Ÿçš„ï¼‰ã€‚varimax ã¯ç›´äº¤å›è»¢ã§ã€å› å­é–“ãŒç„¡ç›¸é–¢ã§ã‚ã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚\nè§£é‡ˆ: å‡ºåŠ›ã•ã‚ŒãŸãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’è¦‹ã¦ã€ã€Œå› å­1ã¯BMIã¨è¡€ç³–å€¤ãŒé«˜ã„ã‹ã‚‰ã€ãƒ¡ã‚¿ãƒœå› å­ã€ã ãªã€ã¨äººé–“ãŒæ„å‘³ä»˜ã‘ã‚’è¡Œã„ã¾ã™ã€‚"
  }
]