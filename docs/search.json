[
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html",
    "title": "SARIMAを使った医療費予測のデモ",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport warnings\n\n# 警告を無視（収束に関する警告などが出る場合があるため）\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#データの準備",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#データの準備",
    "title": "SARIMAを使った医療費予測のデモ",
    "section": "1. データの準備",
    "text": "1. データの準備\n\n# 1. データの作成\nnp.random.seed(42)\n\n# 期間: 過去10年分 (120ヶ月)\ndates = pd.date_range(start='2015-01-01', periods=120, freq='MS')\n\n# トレンド: 徐々に医療費が上がっていく (線形トレンド)\ntrend = np.linspace(100, 150, 120)  # 100単位から1000単位へ上昇 &lt;- ここを変えた。\n\n# 季節性: 12ヶ月周期 (例: 冬に高く、夏に低いなど)\nseasonality = 10 * np.sin(np.linspace(0, 20 * np.pi, 120))\n\n# ノイズ: ランダムな変動\nnoise = np.random.normal(scale=3, size=120)\n\n# 合成して「月次医療費データ」とする\nmedical_costs = trend + seasonality + noise\n\n# DataFrame化\ndf = pd.DataFrame({'Date': dates, 'Cost (100 million)': medical_costs})\ndf.set_index('Date', inplace=True)\ndf[\"Cost (100 million)\"]  = df[\"Cost (100 million)\"].round(2)\n\nprint(\"データの先頭5行:\")\ndisplay(df.head())\n\nデータの先頭5行:\n\n\n\n\n\n\n\n\n\nCost (100 million)\n\n\nDate\n\n\n\n\n\n2015-01-01\n101.49\n\n\n2015-02-01\n187.82\n\n\n2015-03-01\n277.03\n\n\n2015-04-01\n364.15\n\n\n2015-05-01\n440.64"
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#実績データの可視化-plotly",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#実績データの可視化-plotly",
    "title": "SARIMAを使った医療費予測のデモ",
    "section": "実績データの可視化 (Plotly)",
    "text": "実績データの可視化 (Plotly)\n\n# 2. 実績データの可視化\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=df.index,\n    y=df['Cost (100 million)'],\n    mode='lines+markers',\n    name='実績値',\n    line=dict(color='blue')\n))\n\nfig.update_layout(\n    title='医療費ダミーデータ（過去10年）',\n    xaxis_title='年月',\n    yaxis_title='医療費（1億円）',\n    template='plotly_white'\n)\n\nfig.show(renderer=\"notebook\")"
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#sarimaモデルの構築と学習",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#sarimaモデルの構築と学習",
    "title": "SARIMAを使った医療費予測のデモ",
    "section": "SARIMAモデルの構築と学習",
    "text": "SARIMAモデルの構築と学習\n\\(\\phi_p(B) \\Phi_P(B^s) (1 - B)^d (1 - B^s)^D y_t = \\theta_q(B) \\Theta_Q(B^s) \\varepsilon_t\\)\nSARIMAモデル（季節性自己回帰和分移動平均モデル）を構築します。\n※本来はAICなどでパラメータ探索を行いますが、今回はダミーデータの構造に合わせて一般的なパラメータ \\((p, d, q) \\times (P, D, Q, s)\\) を設定します。\n\n# 3. SARIMAモデルの構築\n# order=(p, d, q), seasonal_order=(P, D, Q, s)\n# 周期 s=12 (月次データのため)\nsarima_model = SARIMAX(\n    df['Cost (100 million)'],\n    order=(1, 1, 1),\n    seasonal_order=(1, 1, 1, 12),\n    enforce_stationarity=False,\n    enforce_invertibility=False\n)\n\n# モデルの学習\nsarima_result = sarima_model.fit()\n\nprint(sarima_result.summary())\n\n                                     SARIMAX Results                                      \n==========================================================================================\nDep. Variable:                 Cost (100 million)   No. Observations:                  120\nModel:             SARIMAX(1, 1, 1)x(1, 1, 1, 12)   Log Likelihood                -244.124\nDate:                            Sun, 14 Dec 2025   AIC                            498.249\nTime:                                    15:21:45   BIC                            510.912\nSample:                                01-01-2015   HQIC                           503.362\n                                     - 12-01-2024                                         \nCovariance Type:                              opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.0269      0.121      0.223      0.824      -0.210       0.263\nma.L1         -1.0000    238.268     -0.004      0.997    -467.996     465.996\nar.S.L12      -0.2240      0.144     -1.554      0.120      -0.507       0.058\nma.S.L12      -0.5145      0.166     -3.101      0.002      -0.840      -0.189\nsigma2        10.2597   2444.753      0.004      0.997   -4781.367    4801.887\n===================================================================================\nLjung-Box (L1) (Q):                   0.01   Jarque-Bera (JB):                 0.43\nProb(Q):                              0.91   Prob(JB):                         0.81\nHeteroskedasticity (H):               0.95   Skew:                             0.08\nProb(H) (two-sided):                  0.88   Kurtosis:                         3.29\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step)."
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#向こう5年間の予測と可視化",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#向こう5年間の予測と可視化",
    "title": "SARIMAを使った医療費予測のデモ",
    "section": "向こう5年間の予測と可視化",
    "text": "向こう5年間の予測と可視化\n\n# 4. 向こう5年間 (60ヶ月) の予測\nforecast_steps = 60\npred_uc = sarima_result.get_forecast(steps=forecast_steps)\n\n# 予測値（期待値）\npred_mean = pred_uc.predicted_mean\n\n# 95%信頼区間\npred_ci = pred_uc.conf_int()\n\n# --- 可視化 ---\nfig_forecast = go.Figure()\n\n# 1. 実績値のプロット\nfig_forecast.add_trace(go.Scatter(\n    x=df.index,\n    y=df['Cost (100 million)'],\n    mode='lines',\n    name='実績値',\n    line=dict(color='blue')\n))\n\n# 2. 予測値のプロット\nfig_forecast.add_trace(go.Scatter(\n    x=pred_mean.index,\n    y=pred_mean,\n    mode='lines',\n    name='予測値 (今後5年)',\n    line=dict(color='red', dash='dash')\n))\n\n# 3. 信頼区間のプロット (上限と下限の間を塗りつぶす)\nfig_forecast.add_trace(go.Scatter(\n    x=pred_ci.index,\n    y=pred_ci.iloc[:, 0], # 下限\n    mode='lines',\n    line=dict(width=0),\n    showlegend=False,\n    name='Lower Bound'\n))\n\nfig_forecast.add_trace(go.Scatter(\n    x=pred_ci.index,\n    y=pred_ci.iloc[:, 1], # 上限\n    mode='lines',\n    line=dict(width=0),\n    fill='tonexty', # ひとつ前のトレース（下限）との間を埋める\n    fillcolor='rgba(255, 0, 0, 0.2)', # 赤色の半透明\n    name='95%信頼区間'\n))\n\nfig_forecast.update_layout(\n    title='SARIMAモデルによる医療費予測（向こう5年）',\n    xaxis_title='年月',\n    yaxis_title='医療費',\n    template='plotly_white',\n    hovermode=\"x unified\"\n)\n\nfig_forecast.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "work/statistic_methods/basic_time_series_models/sarima_demo.html#医療費上昇金額の算出",
    "href": "work/statistic_methods/basic_time_series_models/sarima_demo.html#医療費上昇金額の算出",
    "title": "SARIMAを使った医療費予測のデモ",
    "section": "医療費上昇金額の算出",
    "text": "医療費上昇金額の算出\n\n# 5. 上昇金額の算出\n\n# 直近1年間の実績合計\nlast_year_actual = df['Cost (100 million)'].iloc[-12:].sum()\n\n# 5年後（予測期間の最後の1年間）の予測合計\nlast_year_forecast = pred_mean.iloc[-12:].sum()\n\n# 上昇額と上昇率\nincrease_amount = last_year_forecast - last_year_actual\nincrease_rate = (increase_amount / last_year_actual) * 100\n\nprint(f\"--- 医療費上昇シミュレーション ---\")\nprint(f\"直近1年間の医療費合計: {last_year_actual:,.2f}\")\nprint(f\"5年後の年間医療費合計: {last_year_forecast:,.2f}\")\nprint(f\"--------------------------------\")\nprint(f\"年間増加額: +{increase_amount:,.2f}\")\nprint(f\"上昇率: {increase_rate:.2f}%\")\n\n--- 医療費上昇シミュレーション ---\n直近1年間の医療費合計: 1,777.19\n5年後の年間医療費合計: 2,088.13\n--------------------------------\n年間増加額: +310.94\n上昇率: 17.50%"
  },
  {
    "objectID": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html",
    "href": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html",
    "title": "NumPyroを使ってベイズ回帰モデルの実装",
    "section": "",
    "text": "参照させていただいた記事:\n【ベイズ統計モデリング入門】NumPyroで回帰モデルをつくる\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport jax.numpy as jnp\nfrom jax import random, vmap, grad, jit, lax\n\nimport numpyro\nfrom numpyro import plate, sample\nfrom numpyro.infer import MCMC, NUTS, Predictive\nimport numpyro.distributions as dist\n# チップのデータセット\ndf = sns.load_dataset(\"tips\")\n\n# 会計総額とチップのデータを可視化\nsns.set(style=\"darkgrid\")\nsns.jointplot(\n    x=\"total_bill\",\n    y=\"tip\",\n    data=df,\n    kind=\"scatter\",\n    xlim=(0, 60),\n    ylim=(0, 12),\n    color=\"b\",\n    height=7,\n)\ndf.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4"
  },
  {
    "objectID": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html#線形回帰モデル-",
    "href": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html#線形回帰モデル-",
    "title": "NumPyroを使ってベイズ回帰モデルの実装",
    "section": "-線形回帰モデル-",
    "text": "-線形回帰モデル-\n\n\n\nimage.png\n\n\n\n# 線形回帰モデルの設計\ndef model(X, Y=None):\n    pm_a = numpyro.sample(\"pm_a\", dist.Normal(0, 10))\n    pm_b = numpyro.sample(\"pm_b\", dist.Normal(0, 10))\n\n    mu = pm_a * X + pm_b\n    sigma = 1.0\n\n    pm_Y = numpyro.sample(\"pm_Y\", dist.Normal(mu, sigma), obs=Y)\n\n    return pm_Y\n\n\nMCMCの実行\n\n# dfからnumpy.ndarrayに変換\nY = df[\"tip\"].values  # 目的変数：Y = [y0, y1, y2, ... yi]\nX = df[\"total_bill\"].values  # 説明変数：X = [x0, x1, x2, ... xi]\n\n\n# RUN MCMC\n# NUTS (No-U-Turn Sampler) is an adaptive variant of the Hamiltonian Monte Carlo (HMC) algorithm\n# HMCのステップサイズや反復回数などのパラメータを自動調整\n# 効率的なサンプル\n# 収束の改善\n# https://qiita.com/dai08srhg/items/5d4ac3070bae836aef10\nkernel = NUTS(model)\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=2000)\nmcmc.run(rng_key=random.PRNGKey(0), X=X, Y=Y)\n\n# print MCMC summary（推定パラメータの基本統計量）\nmcmc.print_summary()\n\nsample: 100%|██████████| 3000/3000 [00:00&lt;00:00, 3616.90it/s, 11 steps of size 2.54e-01. acc. prob=0.94]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n      pm_a      0.11      0.01      0.11      0.09      0.12    368.76      1.00\n      pm_b      0.90      0.16      0.90      0.65      1.17    377.02      1.00\n\nNumber of divergences: 0\n\n\n\n\n\n\n# get param (num_samples=2000)\nmcmc_samples = mcmc.get_samples()\npm_a = mcmc_samples[\"pm_a\"]\npm_b = mcmc_samples[\"pm_b\"]\n\nprint(\"mcmc_samples:\", mcmc_samples)\nprint(\"pm_a:\", pm_a)\nprint(\"pm_b:\", pm_b)\n\nmcmc_samples: {'pm_a': Array([0.10649762, 0.110033  , 0.10530762, ..., 0.11129791, 0.10644959,\n       0.10579225], dtype=float32), 'pm_b': Array([0.86267745, 0.9398046 , 0.9131396 , ..., 0.751051  , 0.8998211 ,\n       0.8538504 ], dtype=float32)}\npm_a: [0.10649762 0.110033   0.10530762 ... 0.11129791 0.10644959 0.10579225]\npm_b: [0.86267745 0.9398046  0.9131396  ... 0.751051   0.8998211  0.8538504 ]\n\n\n\nlen(set(pm_a.tolist()))\n\n1971\n\n\n\n# 可視化\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.distplot(pm_a)\nplt.title(\"Probability Density of pm_a\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(pm_b)\nplt.title(\"Probability Density of pm_b\")\nplt.show()\n\nC:\\Users\\takuk\\AppData\\Local\\Temp\\ipykernel_27764\\3318693772.py:5: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(pm_a)\nC:\\Users\\takuk\\AppData\\Local\\Temp\\ipykernel_27764\\3318693772.py:9: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(pm_b)\n\n\n\n\n\n\n\n\n\n\nX_range = jnp.linspace(0, 50, 51)\n\npredictive = Predictive(model, mcmc_samples)\npredict_samples = predictive(random.PRNGKey(1), X=X_range, Y=None)\nprint(predict_samples)\n\n{'pm_Y': Array([[ 1.810429  ,  0.02438818,  0.22200845, ...,  5.2724934 ,\n         6.715812  ,  6.422859  ],\n       [ 0.15076251,  0.8095171 ,  0.21540537, ...,  5.2437186 ,\n         7.817451  ,  6.173345  ],\n       [ 2.476837  ,  0.21040367,  0.42815638, ...,  8.04783   ,\n         7.40407   ,  5.8891115 ],\n       ...,\n       [ 1.0230944 ,  1.1552314 ,  0.25326768, ...,  4.831621  ,\n         7.0255423 ,  5.759563  ],\n       [ 1.5060548 , -0.62601656,  1.4508446 , ...,  5.9778643 ,\n         6.058488  ,  4.774219  ],\n       [ 1.0496006 ,  0.51519567,  1.4674567 , ...,  5.349556  ,\n         4.992259  ,  6.651703  ]], dtype=float32)}\n\n\n\npm_Y = predict_samples[\"pm_Y\"]\nprint(pm_Y)\n\n[[ 1.810429    0.02438818  0.22200845 ...  5.2724934   6.715812\n   6.422859  ]\n [ 0.15076251  0.8095171   0.21540537 ...  5.2437186   7.817451\n   6.173345  ]\n [ 2.476837    0.21040367  0.42815638 ...  8.04783     7.40407\n   5.8891115 ]\n ...\n [ 1.0230944   1.1552314   0.25326768 ...  4.831621    7.0255423\n   5.759563  ]\n [ 1.5060548  -0.62601656  1.4508446  ...  5.9778643   6.058488\n   4.774219  ]\n [ 1.0496006   0.51519567  1.4674567  ...  5.349556    4.992259\n   6.651703  ]]\n\n\n\nlen(pm_Y)\n\n2000\n\n\n\n\n推論\n\nmean_Y = pm_Y.mean(axis=0)\ny_low, y_high = jnp.percentile(pm_Y, jnp.array([2.5, 97.5]), axis=0)\n\n# 可視化\nfig = plt.figure(figsize=(6.0, 6.0))\nplt.plot(X_range, mean_Y, \"-\", color=\"g\")\nplt.fill_between(X_range, y_low, y_high, color=\"g\", alpha=0.3)\nplt.plot(X, Y, \"o\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n全てのパラメターを使って推論\n\nN = len(pm_a)\n# X_range = jnp.linspace(0, 50, 50)\n\n# 予測結果の可視化\nfig = plt.figure(figsize=(6.0, 6.0))\nfor i in range(N):\n    pm_y = [pm_a[i] * x + pm_b[i] for x in X_range]\n    plt.plot(X_range, pm_y, \"g-\", alpha=0.5)\n\n# データセットの可視化\nplt.plot(X, Y, \"o\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.title(\"linear regression\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndef my_model(mcmc_samples, X):\n    \"\"\"線形回帰モデル\"\"\"\n    mcmc_samples = mcmc.get_samples()\n    pm_a = mcmc_samples[\"pm_a\"]\n    pm_b = mcmc_samples[\"pm_b\"]\n\n    # 平均値\n    mu_a = jnp.mean(pm_a, axis=0)\n    mu_b = jnp.mean(pm_b, axis=0)\n    print(\"mu_a=\", mu_a)\n    print(\"mu_b=\", mu_b)\n\n    # 線形回帰モデル\n    Y_hat = mu_a * X + mu_b\n\n    return Y_hat\n\n\n# 予測値\nY_hat = my_model(mcmc_samples, X)\n\n# 可視化\nfig = plt.figure(figsize=(6.0, 6.0))\nplt.plot(X, Y, \"o\")\nplt.plot(X, Y_hat, \"g-\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.title(\"linear regression\")\nplt.show()\n\nmu_a= 0.10581156\nmu_b= 0.9027909"
  },
  {
    "objectID": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html#ポアソン回帰モデル",
    "href": "work/statistic_methods/basic_baysian_models/beisyan_modeling.html#ポアソン回帰モデル",
    "title": "NumPyroを使ってベイズ回帰モデルの実装",
    "section": "ポアソン回帰モデル",
    "text": "ポアソン回帰モデル\n\n\n\nimage.png\n\n\n\n# ポアソン回帰モデルの設計\ndef model(X, Y=None):\n    pm_a = numpyro.sample(\"pm_a\", dist.Normal(0.0, 10.0))\n    pm_b = numpyro.sample(\"pm_b\", dist.Normal(0.0, 10.0))\n\n    theta = pm_a * X + pm_b\n    mu = jnp.exp(theta)\n\n    numpyro.sample(\"pm_Y\", dist.Poisson(mu), obs=Y)\n\n\n# Run MCMC\nkernel = NUTS(model)\n# mcmc = MCMC(kernel, num_warmup=1000, num_samples=2000, num_chains=4)\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=2000)\nmcmc.run(rng_key=random.PRNGKey(0), X=X, Y=Y)\n\n# print MCMC summary（事後分布の基本統計量を確認）\nmcmc.print_summary()\n\nsample: 100%|██████████| 3000/3000 [00:00&lt;00:00, 3858.36it/s, 15 steps of size 2.37e-01. acc. prob=0.94]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n      pm_a      0.03      0.00      0.03      0.02      0.04    435.78      1.00\n      pm_b      0.46      0.09      0.45      0.31      0.59    399.54      1.00\n\nNumber of divergences: 0\n\n\n\n\n\n\n# get param (num_samples=2000)\nmcmc_samples = mcmc.get_samples()\npm_a = mcmc_samples[\"pm_a\"]\npm_b = mcmc_samples[\"pm_b\"]\n\nprint(\"mcmc_samples:\", mcmc_samples)\nprint(\"pm_a:\", pm_a)\nprint(\"pm_b:\", pm_b)\n\nmcmc_samples: {'pm_a': Array([0.03003247, 0.0300827 , 0.02773185, ..., 0.03265581, 0.03095045,\n       0.03059688], dtype=float32), 'pm_b': Array([0.5058282 , 0.5151023 , 0.5058284 , ..., 0.37341517, 0.4465436 ,\n       0.42378387], dtype=float32)}\npm_a: [0.03003247 0.0300827  0.02773185 ... 0.03265581 0.03095045 0.03059688]\npm_b: [0.5058282  0.5151023  0.5058284  ... 0.37341517 0.4465436  0.42378387]\n\n\n\n# 可視化\nsns.set(style=\"darkgrid\")\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.distplot(pm_a)\nplt.title(\"Probability Density of pm_a\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(pm_b)\nplt.title(\"Probability Density of pm_b\")\nplt.show()\n\nC:\\Users\\takuk\\AppData\\Local\\Temp\\ipykernel_27764\\3318693772.py:5: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(pm_a)\nC:\\Users\\takuk\\AppData\\Local\\Temp\\ipykernel_27764\\3318693772.py:9: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(pm_b)\n\n\n\n\n\n\n\n\n\n\n推論\n\nX_range = jnp.linspace(0, 50, 50)\n\npredictive = Predictive(model, mcmc_samples)\npredict_samples = predictive(random.PRNGKey(0), X=X_range, Y=None)\n# print(predict_samples)\n\npm_Y = predict_samples[\"pm_Y\"]\n# print(pm_Y)\n\nmean_Y = pm_Y.mean(axis=0)\ny_low, y_high = jnp.percentile(pm_Y, jnp.array([2.5, 97.5]), axis=0)\n\n# 可視化\nfig = plt.figure(figsize=(6.0, 6.0))\nplt.plot(X_range, mean_Y, \"-\", color=\"g\")\nplt.fill_between(X_range, y_low, y_high, color=\"g\", alpha=0.3)\nplt.plot(X, Y, \"o\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n全ての推定パラメータを使って推論\n\nN = len(pm_a)\n# X_range = jnp.linspace(0, 50, 50)\n\n# 予測結果の可視化\nfig = plt.figure(figsize=(6.0, 6.0))\nfor i in range(N):\n    pm_y = [jnp.exp(pm_a[i] * x + pm_b[i]) for x in X_range]\n    plt.plot(X_range, pm_y, \"g-\", alpha=0.5)\n\n# データセットの可視化\nplt.plot(X, Y, \"o\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.title(\"linear regression\")\nplt.show()\n\n\n\n\n\n\n\n\n\ndef my_model(mcmc_samples, X):\n    \"\"\"ポアソン回帰モデル\"\"\"\n    mcmc_samples = mcmc.get_samples()\n    pm_a = mcmc_samples[\"pm_a\"]\n    pm_b = mcmc_samples[\"pm_b\"]\n\n    # 平均値\n    mu_a = jnp.mean(pm_a, axis=0)\n    mu_b = jnp.mean(pm_b, axis=0)\n    print(\"mu_a=\", mu_a)\n    print(\"mu_b=\", mu_b)\n\n    # ポアソン回帰モデル\n    Y_hat = jnp.exp(mu_a * X + mu_b)\n\n    return Y_hat\n\n\n# 予測値\nY_hat = my_model(mcmc_samples, X_range)\n\n# 可視化\nfig = plt.figure(figsize=(6.0, 6.0))\nplt.plot(X, Y, \"o\")\nplt.plot(X_range, Y_hat, \"g-\")\nplt.xlabel(\"x\"), plt.ylabel(\"y\")\nplt.title(\"poisson regression\")\nplt.show()\n\nmu_a= 0.030312581\nmu_b= 0.45623016"
  },
  {
    "objectID": "work/improve_python/demo_batch.html",
    "href": "work/improve_python/demo_batch.html",
    "title": "Python標準ライブラリーでバッチ処理を行う方法",
    "section": "",
    "text": "import pandas as pd\nimport duckdb\nimport faker\n\nfrom itertools import batched\n\n\ndata = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n\n# 3個ずつのバッチに分割\nfor batch in batched(data, 3):\n    print(batch)\n\n# 出力:\n# ('A', 'B', 'C')\n# ('D', 'E', 'F')\n# ('G',)  &lt;- 余った分も最後に出力される\n\n('A', 'B', 'C')\n('D', 'E', 'F')\n('G',)\n\n\n\n# fakerを使って100万行のダミーデータを生成し、DuckDBに挿入\nfake = faker.Faker()\n\n\n%%time\n\ndata = [{\n    \"name\":fake.name(),\n    \"sex\":fake.random_element(elements=[\"M\", \"F\"]),\n    \"age\":fake.random_int(min=18, max=80),\n    \"address\":fake.address().replace(\"\\n\", \", \"),\n    \"phone_number\":fake.phone_number(),\n} for i in range(1_000_000)\n]\n\ndf = pd.DataFrame(data)\n# display(df.head())\n# df.shape\n\nCPU times: total: 3min 54s\nWall time: 4min\n\n\n\n%%time\n\n# ジェネレータ関数で無限にデータを生成できるようにしておく\ndef fake_data_generator():\n    while True:\n        yield {\n            \"name\":fake.name(),\n            \"sex\":fake.random_element(elements=[\"M\", \"F\"]),\n            \"age\":fake.random_int(min=18, max=80),\n            \"address\":fake.address().replace(\"\\n\", \", \"),\n            \"phone_number\":fake.phone_number(),\n        }\n\nall_dfs = []\nfor batch in batched(fake_data_generator(), 10000): # 1万件ずつバッチ処理\n    batch_df = pd.DataFrame(batch)\n    all_dfs.append(batch_df)\n    \n    if len(all_dfs) &gt;= 100: # 10000 * 100 = 1_000_000件でストップ\n        break\n\ndf = pd.concat(all_dfs)\n\nCPU times: total: 2min 1s\nWall time: 2min 1s\n\n\n\noutput_csv = \"fake_data_100k.csv\"\ndf.to_csv(output_csv, index=False)\n\n\n%%timeit\nreader = pd.read_csv(output_csv, chunksize=10000)\n\ndf_tmps = []\nfor df_chunk in reader:\n    df_tmps.append(df_chunk)\ndf_tmp = pd.concat(df_tmps)\n\n# df_tmp.head()\n\n1.02 s ± 42.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%%timeit\ndf_tmp = pd.read_csv(output_csv)\n\n821 ms ± 9.34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nimport time\n\nuser_ids = range(1000)\nfor batch in batched(user_ids, 50, strict=False):\n    print(batch)\n    time.sleep(1) # 負荷軽減のための待機\n\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49)\n(50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99)\n(100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149)\n(150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199)\n(200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249)\n(250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299)\n(300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349)\n(350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399)\n(400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449)\n(450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499)\n(500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549)\n(550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599)\n(600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649)\n(650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699)\n(700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749)\n(750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799)\n(800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849)\n(850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899)\n(900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949)\n(950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999)"
  },
  {
    "objectID": "work/improve_python/data/titanic.html",
    "href": "work/improve_python/data/titanic.html",
    "title": "Tako Data Science Lab",
    "section": "",
    "text": "survived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n3\nmale\n22\n1\n0\n7.25\nS\nThird\nman\nTrue\nnan\nSouthampton\nno\nFalse\n\n\n1\n1\nfemale\n38\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n1\n3\nfemale\n26\n0\n0\n7.925\nS\nThird\nwoman\nFalse\nnan\nSouthampton\nyes\nTrue\n\n\n1\n1\nfemale\n35\n1\n0\n53.1\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n0\n3\nmale\n35\n0\n0\n8.05\nS\nThird\nman\nTrue\nnan\nSouthampton\nno\nTrue\n\n\n0\n3\nmale\nnan\n0\n0\n8.4583\nQ\nThird\nman\nTrue\nnan\nQueenstown\nno\nTrue\n\n\n0\n1\nmale\n54\n0\n0\n51.8625\nS\nFirst\nman\nTrue\nE\nSouthampton\nno\nTrue\n\n\n0\n3\nmale\n2\n3\n1\n21.075\nS\nThird\nchild\nFalse\nnan\nSouthampton\nno\nFalse\n\n\n1\n3\nfemale\n27\n0\n2\n11.1333\nS\nThird\nwoman\nFalse\nnan\nSouthampton\nyes\nFalse\n\n\n1\n2\nfemale\n14\n1\n0\n30.0708\nC\nSecond\nchild\nFalse\nnan\nCherbourg\nyes\nFalse"
  },
  {
    "objectID": "work/improve_python/data/orders.html",
    "href": "work/improve_python/data/orders.html",
    "title": "Tako Data Science Lab",
    "section": "",
    "text": "user_id\nitem\nprice\n\n\n\n\n1\nリンゴ\n100\n\n\n2\nバナナ\n200\n\n\n2\nメロン\n300\n\n\n4\nブドウ\n400"
  },
  {
    "objectID": "work/improve_python/advanced_pandas.html",
    "href": "work/improve_python/advanced_pandas.html",
    "title": "【脱初心者】Pandasのおしゃな書き方を目指して",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom modules.generate_data import generate_survey_data, generate_duplicate_example\n# バージョンの確認\nprint(pd.__version__)\n\n3.0.0\ndf = sns.load_dataset('titanic')\ndisplay(df.head())\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\ndf_q = generate_survey_data(n=10)\ndisplay(df_q)\n\n\n\n\n\n\n\n\nユーザーID\n年齢\n満足度\n好きな食べ物\n\n\n\n\n0\nU001\n32\n2.0\n[寿司, カレー, ラーメン]\n\n\n1\nU002\n40\n5.0\n[ラーメン]\n\n\n2\nU003\n29\nNaN\n[サラダ, 寿司, パスタ]\n\n\n3\nU004\n49\n4.0\n[ハンバーグ]\n\n\n4\nU005\n39\n5.0\n[パスタ, ラーメン, ハンバーグ]\n\n\n5\nU006\n47\n4.0\n[寿司, ハンバーグ]\n\n\n6\nU007\n57\nNaN\n[カレー, ラーメン]\n\n\n7\nU008\n20\n2.0\n[ラーメン, パスタ, カレー]\n\n\n8\nU009\n31\n2.0\n[ハンバーグ, 寿司, ラーメン]\n\n\n9\nU010\n33\n4.0\n[パスタ, 寿司, ラーメン]\n# user_idで結合する多対多になるデータを用意\ndf_user, df_order = generate_duplicate_example()\ndisplay(df_user)\ndisplay(df_order)\n\n\n\n\n\n\n\n\nuser_id\nuser_name\n\n\n\n\n0\n1\n田中\n\n\n1\n2\n佐藤_旧\n\n\n2\n2\n佐藤_新\n\n\n3\n3\n鈴木\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nitem\nprice\n\n\n\n\n0\n1\nリンゴ\n100\n\n\n1\n2\nバナナ\n200\n\n\n2\n2\nメロン\n300\n\n\n3\n4\nブドウ\n400\ndata = {\n    \"カテゴリ\": [],\n    \"コード\": [],\n    \"用途\": [],\n    \"使用データ\": [],\n}\n\ndef add_entry(category, code, purpose, used_data):\n    data[\"カテゴリ\"].append(category)\n    data[\"コード\"].append(code)\n    data[\"用途\"].append(purpose)\n    data[\"使用データ\"].append(used_data)"
  },
  {
    "objectID": "work/improve_python/advanced_pandas.html#データ読み込みと出力",
    "href": "work/improve_python/advanced_pandas.html#データ読み込みと出力",
    "title": "【脱初心者】Pandasのおしゃな書き方を目指して",
    "section": "データ読み込みと出力",
    "text": "データ読み込みと出力\n\nadd_entry(\n    category=\"読込\",\n    code=\"pd.read_csv(path, chunksize=100, usecols=[...])\",\n    purpose=\"大容量のCSVファイルをメモリを節約しながら読み込む\",\n    used_data=\"titanic\",\n)\n\ndf_lst = []\nchunks = pd.read_csv(\"./data/titanic.csv\", chunksize=100, usecols=['survived', 'pclass', 'fare'])\nfor chunk in chunks:\n    optimized_chunk = chunk.convert_dtypes()\n    agged = optimized_chunk.groupby(['survived', 'pclass']).agg(\n        fare=('fare', 'sum'),\n        user=('fare', 'count')\n    ).reset_index()\n    df_lst.append(agged)\n    \ndf_final = pd.concat(df_lst, ignore_index=True)\nagged = df_final.groupby(['survived', 'pclass']).agg(\n    fare=('fare', 'sum'),\n    user=('user', 'sum')\n)\nagged[\"mean_fare\"] = agged[\"fare\"] / agged[\"user\"]\ndisplay(agged)\n\n\n\n\n\n\n\n\n\nfare\nuser\nmean_fare\n\n\nsurvived\npclass\n\n\n\n\n\n\n\n0\n1\n5174.7206\n80\n64.684007\n\n\n2\n1882.9958\n97\n19.412328\n\n\n3\n5085.0035\n372\n13.669364\n\n\n1\n1\n13002.6919\n136\n95.608029\n\n\n2\n1918.8459\n87\n22.0557\n\n\n3\n1629.6916\n119\n13.694887\n\n\n\n\n\n\n\nちょっと蛇足化もしれないがメモリを節約することができる\n\n# マークダウンで州つりょく\n\nadd_entry(\n    category=\"出力\",\n    code=\"df.to_markdown('path')\",\n    purpose=\"データをマークダウン形式で出力する\",\n    used_data=\"titanic, survey, users, orders\",\n)\n\n\ndf.head(10).to_markdown(\"./data/titanic.md\", index=False)\ndf_q.head(10).to_markdown(\"./data/survey.md\", index=False)\ndf_user.head(10).to_markdown(\"./data/users.md\", index=False)\ndf_order.head(10).to_markdown(\"./data/orders.md\", index=False)\n# df.to_csv(\"./data/titanic.csv\", index=False)\n\n\nadd_entry(\n    category=\"出力\",\n    code=\"df.style.background_gradient(path)\",\n    purpose=\"データをヒートマップで可視化する\",\n    used_data=\"titanic\",\n)\n\n# 簡単なヒートマップはpanadsだけで可能\nagged.style.background_gradient(cmap='Blues', subset=['mean_fare'])\n\n\n\n\n\n\n \n \nfare\nuser\nmean_fare\n\n\nsurvived\npclass\n \n \n \n\n\n\n\n0\n1\n5174.720600\n80\n64.684007\n\n\n2\n1882.995800\n97\n19.412328\n\n\n3\n5085.003500\n372\n13.669364\n\n\n1\n1\n13002.691900\n136\n95.608029\n\n\n2\n1918.845900\n87\n22.055700\n\n\n3\n1629.691600\n119\n13.694887"
  },
  {
    "objectID": "work/improve_python/advanced_pandas.html#データ抽出",
    "href": "work/improve_python/advanced_pandas.html#データ抽出",
    "title": "【脱初心者】Pandasのおしゃな書き方を目指して",
    "section": "データ抽出",
    "text": "データ抽出\n\nadd_entry(\n    category=\"抽出\",\n    code=\"df.nlargest(n=10, columns='fare')\",\n    purpose=\"指定したカラムのデータを大きい順に抽出する\",\n    used_data=\"titanic\",\n)\n\n# 指定したカラムのデータを大きい順に抽出\ndf.nlargest(n=10, columns=['fare', \"pclass\"])\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n258\n1\n1\nfemale\n35.0\n0\n0\n512.3292\nC\nFirst\nwoman\nFalse\nNaN\nCherbourg\nyes\nTrue\n\n\n679\n1\n1\nmale\n36.0\n0\n1\n512.3292\nC\nFirst\nman\nTrue\nB\nCherbourg\nyes\nFalse\n\n\n737\n1\n1\nmale\n35.0\n0\n0\n512.3292\nC\nFirst\nman\nTrue\nB\nCherbourg\nyes\nTrue\n\n\n27\n0\n1\nmale\n19.0\n3\n2\n263.0000\nS\nFirst\nman\nTrue\nC\nSouthampton\nno\nFalse\n\n\n88\n1\n1\nfemale\n23.0\n3\n2\n263.0000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n341\n1\n1\nfemale\n24.0\n3\n2\n263.0000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n438\n0\n1\nmale\n64.0\n1\n4\n263.0000\nS\nFirst\nman\nTrue\nC\nSouthampton\nno\nFalse\n\n\n311\n1\n1\nfemale\n18.0\n2\n2\n262.3750\nC\nFirst\nwoman\nFalse\nB\nCherbourg\nyes\nFalse\n\n\n742\n1\n1\nfemale\n21.0\n2\n2\n262.3750\nC\nFirst\nwoman\nFalse\nB\nCherbourg\nyes\nFalse\n\n\n118\n0\n1\nmale\n24.0\n0\n1\n247.5208\nC\nFirst\nman\nTrue\nB\nCherbourg\nno\nFalse\n\n\n\n\n\n\n\n\nadd_entry(\n    category=\"抽出\",\n    code=\"df.nsmallest(n=10, columns='fare')\",\n    purpose=\"指定したカラムのデータを小さい順に抽出する\",\n    used_data=\"titanic\",\n)\n\n# 小さい順に抽出\ndf.nsmallest(n=10, columns='fare')\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n179\n0\n3\nmale\n36.0\n0\n0\n0.0\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n263\n0\n1\nmale\n40.0\n0\n0\n0.0\nS\nFirst\nman\nTrue\nB\nSouthampton\nno\nTrue\n\n\n271\n1\n3\nmale\n25.0\n0\n0\n0.0\nS\nThird\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n277\n0\n2\nmale\nNaN\n0\n0\n0.0\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n302\n0\n3\nmale\n19.0\n0\n0\n0.0\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n413\n0\n2\nmale\nNaN\n0\n0\n0.0\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n466\n0\n2\nmale\nNaN\n0\n0\n0.0\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n481\n0\n2\nmale\nNaN\n0\n0\n0.0\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n597\n0\n3\nmale\n49.0\n0\n0\n0.0\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n633\n0\n1\nmale\nNaN\n0\n0\n0.0\nS\nFirst\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\nadd_entry(\n    category=\"抽出\",\n    code=\"df.select_dtypes(include='データタイプ')\",\n    purpose=\"特定のデータ型のカラムのみを抽出する\",\n    used_data=\"titanic\",\n)\n\n# 型で抽出するデータを選べる\nnumbers = df.select_dtypes(include='number')\ndisplay(numbers.head())\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\n0\n0\n3\n22.0\n1\n0\n7.2500\n\n\n1\n1\n1\n38.0\n1\n0\n71.2833\n\n\n2\n1\n3\n26.0\n0\n0\n7.9250\n\n\n3\n1\n1\n35.0\n1\n0\n53.1000\n\n\n4\n0\n3\n35.0\n0\n0\n8.0500\n\n\n\n\n\n\n\n\nnumbers.hist()\nplt.tight_layout()"
  },
  {
    "objectID": "work/improve_python/advanced_pandas.html#データ型",
    "href": "work/improve_python/advanced_pandas.html#データ型",
    "title": "【脱初心者】Pandasのおしゃな書き方を目指して",
    "section": "データ型",
    "text": "データ型\n\ndf.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    str     \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    str     \n 8   class        891 non-null    category\n 9   who          891 non-null    str     \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    str     \n 13  alive        891 non-null    str     \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), str(5)\nmemory usage: 80.7 KB\n\n\n\nadd_entry(\n    category=\"データ型\",\n    code=\"convert_dtypes()\",\n    purpose=\"データ型を柔軟に変換する\",\n    used_data=\"titanic\",\n)\n\n# 柔軟な型に変換してくれる\ndf.convert_dtypes().info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    Int64   \n 1   pclass       891 non-null    Int64   \n 2   sex          891 non-null    string  \n 3   age          714 non-null    Float64 \n 4   sibsp        891 non-null    Int64   \n 5   parch        891 non-null    Int64   \n 6   fare         891 non-null    Float64 \n 7   embarked     889 non-null    string  \n 8   class        891 non-null    category\n 9   who          891 non-null    string  \n 10  adult_male   891 non-null    boolean \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    string  \n 13  alive        891 non-null    string  \n 14  alone        891 non-null    boolean \ndtypes: Float64(2), Int64(4), boolean(2), category(2), string(5)\nmemory usage: 87.6 KB\n\n\n\n# 特定の型を変換しないように指定も可能\ndf.convert_dtypes(\n    convert_integer=False,\n    convert_boolean=False,\n    convert_floating=False,\n    convert_string=True,\n).info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    string  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    string  \n 8   class        891 non-null    category\n 9   who          891 non-null    string  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    string  \n 13  alive        891 non-null    string  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), string(5)\nmemory usage: 80.7 KB"
  },
  {
    "objectID": "work/improve_python/advanced_pandas.html#整形",
    "href": "work/improve_python/advanced_pandas.html#整形",
    "title": "【脱初心者】Pandasのおしゃな書き方を目指して",
    "section": "整形",
    "text": "整形\n\n# 年齢をビン分割してカテゴリ変数に変換\n\nadd_entry(\n    category=\"整形\",\n    code=\"pd.cut(Series, bins=[], labels=[])\",\n    purpose=\"範囲を指定して数値データをカテゴリ変数に変換する\",\n    used_data=\"titanic\",\n)\n\ndf[\"age_group\"] = pd.cut(\n    df[\"age\"], \n    bins=[0, 12, 20, 40, 60, 80, np.inf], \n    labels=[\"child\", \"teen\", \"adult\", \"middle_aged\", \"senior\", \"elderly\"],\n    \n)\ndf[[\"age\", \"age_group\"]].head(10)\n\n\n\n\n\n\n\n\nage\nage_group\n\n\n\n\n0\n22.0\nadult\n\n\n1\n38.0\nadult\n\n\n2\n26.0\nadult\n\n\n3\n35.0\nadult\n\n\n4\n35.0\nadult\n\n\n5\nNaN\nNaN\n\n\n6\n54.0\nmiddle_aged\n\n\n7\n2.0\nchild\n\n\n8\n27.0\nadult\n\n\n9\n14.0\nteen\n\n\n\n\n\n\n\n\ndf.query(\"0 &lt;= age &lt;= 12\")[\"age_group\"].unique()\n\n['child']\nCategories (6, str): ['child' &lt; 'teen' &lt; 'adult' &lt; 'middle_aged' &lt; 'senior' &lt; 'elderly']\n\n\nカテゴリ変数となり順序の大小が確保される\n\nadd_entry(\n    category=\"整形\",\n    code=\"pd.qcut(Series, q=4, labels=[])\",\n    purpose=\"分割数を決めて数値データをカテゴリ変数に変換する\",\n    used_data=\"titanic\",\n)\n\n# 料金を4分割してカテゴリ変数に変換\ndf[\"fare_rank\"] = pd.qcut(df[\"fare\"], q=4, labels=[\"low\", \"medium\", \"high\", \"very_high\"])\ndf[[\"fare\", \"fare_rank\"]]\n\n\n\n\n\n\n\n\nfare\nfare_rank\n\n\n\n\n0\n7.2500\nlow\n\n\n1\n71.2833\nvery_high\n\n\n2\n7.9250\nmedium\n\n\n3\n53.1000\nvery_high\n\n\n4\n8.0500\nmedium\n\n\n...\n...\n...\n\n\n886\n13.0000\nmedium\n\n\n887\n30.0000\nhigh\n\n\n888\n23.4500\nhigh\n\n\n889\n30.0000\nhigh\n\n\n890\n7.7500\nlow\n\n\n\n\n891 rows × 2 columns\n\n\n\n\ndf_q\n\n\n\n\n\n\n\n\nユーザーID\n年齢\n満足度\n好きな食べ物\n\n\n\n\n0\nU001\n32\n2.0\n[寿司, カレー, ラーメン]\n\n\n1\nU002\n40\n5.0\n[ラーメン]\n\n\n2\nU003\n29\nNaN\n[サラダ, 寿司, パスタ]\n\n\n3\nU004\n49\n4.0\n[ハンバーグ]\n\n\n4\nU005\n39\n5.0\n[パスタ, ラーメン, ハンバーグ]\n\n\n5\nU006\n47\n4.0\n[寿司, ハンバーグ]\n\n\n6\nU007\n57\nNaN\n[カレー, ラーメン]\n\n\n7\nU008\n20\n2.0\n[ラーメン, パスタ, カレー]\n\n\n8\nU009\n31\n2.0\n[ハンバーグ, 寿司, ラーメン]\n\n\n9\nU010\n33\n4.0\n[パスタ, 寿司, ラーメン]\n\n\n\n\n\n\n\n\nadd_entry(\n    category=\"整形\",\n    code=\"df.explode(column_name)\",\n    purpose=\"リスト型のデータを展開して縦長にする\",\n    used_data=\"survey\",\n)\n\n# explodeメソッドでリストを展開\ndf_q_exploded = df_q.explode([\"好きな食べ物\"]).reset_index(drop=True)\ndf_q_exploded\n\n\n\n\n\n\n\n\nユーザーID\n年齢\n満足度\n好きな食べ物\n\n\n\n\n0\nU001\n32\n2.0\n寿司\n\n\n1\nU001\n32\n2.0\nカレー\n\n\n2\nU001\n32\n2.0\nラーメン\n\n\n3\nU002\n40\n5.0\nラーメン\n\n\n4\nU003\n29\nNaN\nサラダ\n\n\n5\nU003\n29\nNaN\n寿司\n\n\n6\nU003\n29\nNaN\nパスタ\n\n\n7\nU004\n49\n4.0\nハンバーグ\n\n\n8\nU005\n39\n5.0\nパスタ\n\n\n9\nU005\n39\n5.0\nラーメン\n\n\n10\nU005\n39\n5.0\nハンバーグ\n\n\n11\nU006\n47\n4.0\n寿司\n\n\n12\nU006\n47\n4.0\nハンバーグ\n\n\n13\nU007\n57\nNaN\nカレー\n\n\n14\nU007\n57\nNaN\nラーメン\n\n\n15\nU008\n20\n2.0\nラーメン\n\n\n16\nU008\n20\n2.0\nパスタ\n\n\n17\nU008\n20\n2.0\nカレー\n\n\n18\nU009\n31\n2.0\nハンバーグ\n\n\n19\nU009\n31\n2.0\n寿司\n\n\n20\nU009\n31\n2.0\nラーメン\n\n\n21\nU010\n33\n4.0\nパスタ\n\n\n22\nU010\n33\n4.0\n寿司\n\n\n23\nU010\n33\n4.0\nラーメン\n\n\n\n\n\n\n\n複数列の同時展開はリスト内の要素数が同じじゃないとできない。\nついでにstack, unstack, meltも見てみる\n\nstackは横長のデータを縦長に積み上げる(インデックスが設定されている)\nmeltも横長のデータを縦長に積み上げる(特定の列を指定して固定し、そのほかを縦に変換)\nunstackは縦長のデータを横長にする\npivotは列の値を新しい列名にする（重複無し）\npivot_tableはpivotに集計をプラスする\n\n\nadd_entry(\n    category=\"整形\",\n    code=\"df.explode(column_name)\",\n    purpose=\"リスト型のデータを展開して縦長にする\",\n    used_data=\"titanic\",\n)\n\ndf_single_columns = df[[\"embark_town\", \"fare\"]]\ndf_single_columns.stack(level=-1).reset_index().head(10)\n\n\n\n\n\n\n\n\nlevel_0\nlevel_1\n0\n\n\n\n\n0\n0\nembark_town\nSouthampton\n\n\n1\n0\nfare\n7.25\n\n\n2\n1\nembark_town\nCherbourg\n\n\n3\n1\nfare\n71.2833\n\n\n4\n2\nembark_town\nSouthampton\n\n\n5\n2\nfare\n7.925\n\n\n6\n3\nembark_town\nSouthampton\n\n\n7\n3\nfare\n53.1\n\n\n8\n4\nembark_town\nSouthampton\n\n\n9\n4\nfare\n8.05\n\n\n\n\n\n\n\n\nagged = (\n    df.groupby([\"class\"])\n    .agg(\n        average_fare=pd.NamedAgg(column=\"fare\", aggfunc=\"mean\"),\n        max_fare=pd.NamedAgg(column=\"fare\", aggfunc=\"max\"),\n        min_fare=pd.NamedAgg(column=\"fare\", aggfunc=\"min\"),\n        avg_age=pd.NamedAgg(column=\"age\", aggfunc=\"mean\"),\n        sruvived_rate=pd.NamedAgg(column=\"survived\", aggfunc=\"mean\"),\n    )\n)\nagged\n\n\n\n\n\n\n\n\naverage_fare\nmax_fare\nmin_fare\navg_age\nsruvived_rate\n\n\nclass\n\n\n\n\n\n\n\n\n\nFirst\n84.154687\n512.3292\n0.0\n38.233441\n0.629630\n\n\nSecond\n20.662183\n73.5000\n0.0\n29.877630\n0.472826\n\n\nThird\n13.675550\n69.5500\n0.0\n25.140620\n0.242363\n\n\n\n\n\n\n\n\nstack_se = agged.stack(level=0)\nstack_se\n\nclass                \nFirst   average_fare      84.154687\n        max_fare         512.329200\n        min_fare           0.000000\n        avg_age           38.233441\n        sruvived_rate      0.629630\nSecond  average_fare      20.662183\n        max_fare          73.500000\n        min_fare           0.000000\n        avg_age           29.877630\n        sruvived_rate      0.472826\nThird   average_fare      13.675550\n        max_fare          69.550000\n        min_fare           0.000000\n        avg_age           25.140620\n        sruvived_rate      0.242363\ndtype: float64\n\n\n\nstack_se.reset_index()\n\n\n\n\n\n\n\n\nclass\nlevel_1\n0\n\n\n\n\n0\nFirst\naverage_fare\n84.154687\n\n\n1\nFirst\nmax_fare\n512.329200\n\n\n2\nFirst\nmin_fare\n0.000000\n\n\n3\nFirst\navg_age\n38.233441\n\n\n4\nFirst\nsruvived_rate\n0.629630\n\n\n5\nSecond\naverage_fare\n20.662183\n\n\n6\nSecond\nmax_fare\n73.500000\n\n\n7\nSecond\nmin_fare\n0.000000\n\n\n8\nSecond\navg_age\n29.877630\n\n\n9\nSecond\nsruvived_rate\n0.472826\n\n\n10\nThird\naverage_fare\n13.675550\n\n\n11\nThird\nmax_fare\n69.550000\n\n\n12\nThird\nmin_fare\n0.000000\n\n\n13\nThird\navg_age\n25.140620\n\n\n14\nThird\nsruvived_rate\n0.242363\n\n\n\n\n\n\n\n横長になった集計値を確認するにはちょうどよさそう。\n\nstack_se.index\n\nMultiIndex([( 'First',  'average_fare'),\n            ( 'First',      'max_fare'),\n            ( 'First',      'min_fare'),\n            ( 'First',       'avg_age'),\n            ( 'First', 'sruvived_rate'),\n            ('Second',  'average_fare'),\n            ('Second',      'max_fare'),\n            ('Second',      'min_fare'),\n            ('Second',       'avg_age'),\n            ('Second', 'sruvived_rate'),\n            ( 'Third',  'average_fare'),\n            ( 'Third',      'max_fare'),\n            ( 'Third',      'min_fare'),\n            ( 'Third',       'avg_age'),\n            ( 'Third', 'sruvived_rate')],\n           names=['class', None])\n\n\n\n# stackで積み上げられたものはunstackで元に戻せる\nunstacked = stack_se.unstack(level=1)\nunstacked\n\n\n\n\n\n\n\n\naverage_fare\nmax_fare\nmin_fare\navg_age\nsruvived_rate\n\n\nclass\n\n\n\n\n\n\n\n\n\nFirst\n84.154687\n512.3292\n0.0\n38.233441\n0.629630\n\n\nSecond\n20.662183\n73.5000\n0.0\n29.877630\n0.472826\n\n\nThird\n13.675550\n69.5500\n0.0\n25.140620\n0.242363\n\n\n\n\n\n\n\n\nadd_entry(\n    category=\"整形\",\n    code=\"df.melt(id_vars=[], var_name='', value_name='')\",\n    purpose=\"データを横長から縦長に変換する\",\n    used_data=\"titanic\",\n)\n\n# meltメソッドで縦に長いデータに変換\ndf_agged = agged.reset_index()\ndf_agged.melt(id_vars=[\"class\"], var_name=\"指標\", value_name=\"値\")\n\n\n\n\n\n\n\n\nclass\n指標\n値\n\n\n\n\n0\nFirst\naverage_fare\n84.154687\n\n\n1\nSecond\naverage_fare\n20.662183\n\n\n2\nThird\naverage_fare\n13.675550\n\n\n3\nFirst\nmax_fare\n512.329200\n\n\n4\nSecond\nmax_fare\n73.500000\n\n\n5\nThird\nmax_fare\n69.550000\n\n\n6\nFirst\nmin_fare\n0.000000\n\n\n7\nSecond\nmin_fare\n0.000000\n\n\n8\nThird\nmin_fare\n0.000000\n\n\n9\nFirst\navg_age\n38.233441\n\n\n10\nSecond\navg_age\n29.877630\n\n\n11\nThird\navg_age\n25.140620\n\n\n12\nFirst\nsruvived_rate\n0.629630\n\n\n13\nSecond\nsruvived_rate\n0.472826\n\n\n14\nThird\nsruvived_rate\n0.242363"
  },
  {
    "objectID": "work/improve_python/advanced_pandas.html#結合の安全性",
    "href": "work/improve_python/advanced_pandas.html#結合の安全性",
    "title": "【脱初心者】Pandasのおしゃな書き方を目指して",
    "section": "結合の安全性",
    "text": "結合の安全性\n\ndisplay(df_user)\ndisplay(df_order)\n\n\n\n\n\n\n\n\nuser_id\nuser_name\n\n\n\n\n0\n1\n田中\n\n\n1\n2\n佐藤_旧\n\n\n2\n2\n佐藤_新\n\n\n3\n3\n鈴木\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nitem\nprice\n\n\n\n\n0\n1\nリンゴ\n100\n\n\n1\n2\nバナナ\n200\n\n\n2\n2\nメロン\n300\n\n\n3\n4\nブドウ\n400\n\n\n\n\n\n\n\n\n# 普通にマージすると気づかずしてしまう。\ndf_merged = pd.merge(df_user, df_order, on='user_id', how='inner')\ndisplay(df_merged)\n\n\n\n\n\n\n\n\nuser_id\nuser_name\nitem\nprice\n\n\n\n\n0\n1\n田中\nリンゴ\n100\n\n\n1\n2\n佐藤_旧\nバナナ\n200\n\n\n2\n2\n佐藤_旧\nメロン\n300\n\n\n3\n2\n佐藤_新\nバナナ\n200\n\n\n4\n2\n佐藤_新\nメロン\n300\n\n\n\n\n\n\n\n\nadd_entry(\n    category=\"結合\",\n    code=\"df.merge(other_df, on='column_name', how='inner', validate='one_to_many')\",\n    purpose=\"結合前にデータの関係性を検証する\",\n    used_data=\"users, orders\",\n)\n\ndf_merged = pd.merge(\n    df_user, \n    df_order,\n    on='user_id',\n    how='inner',\n    validate='one_to_many' # これを追加\n)\ndisplay(df_merged)\n\n\n---------------------------------------------------------------------------\nMergeError                                Traceback (most recent call last)\nCell In[31], line 8\n      1 add_entry(\n      2     category=\"結合\",\n      3     code=\"df.merge(other_df, on='column_name', how='inner', validate='one_to_many')\",\n      4     purpose=\"結合前にデータの関係性を検証する\",\n      5     used_data=\"users, orders\",\n      6 )\n----&gt; 8 df_merged = pd.merge(\n      9     df_user, \n     10     df_order,\n     11     on='user_id',\n     12     how='inner',\n     13     validate='one_to_many' # これを追加\n     14 )\n     15 display(df_merged)\n\nFile d:\\data-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:385, in merge(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\n    371     return _cross_merge(\n    372         left_df,\n    373         right_df,\n   (...)    382         validate=validate,\n    383     )\n    384 else:\n--&gt; 385     op = _MergeOperation(\n    386         left_df,\n    387         right_df,\n    388         how=how,\n    389         on=on,\n    390         left_on=left_on,\n    391         right_on=right_on,\n    392         left_index=left_index,\n    393         right_index=right_index,\n    394         sort=sort,\n    395         suffixes=suffixes,\n    396         indicator=indicator,\n    397         validate=validate,\n    398     )\n    399     return op.get_result()\n\nFile d:\\data-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1037, in _MergeOperation.__init__(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\n   1033 # If argument passed to validate,\n   1034 # check if columns specified as unique\n   1035 # are in fact unique.\n   1036 if validate is not None:\n-&gt; 1037     self._validate_validate_kwd(validate)\n\nFile d:\\data-analysis\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2012, in _MergeOperation._validate_validate_kwd(self, validate)\n   2010 elif validate in [\"one_to_many\", \"1:m\"]:\n   2011     if not left_unique:\n-&gt; 2012         raise MergeError(\n   2013             \"Merge keys are not unique in left dataset; not a one-to-many merge\"\n   2014             f\"{left_error_msg(left_join_index)}\"\n   2015         )\n   2017 elif validate in [\"many_to_one\", \"m:1\"]:\n   2018     if not right_unique:\n\nMergeError: Merge keys are not unique in left dataset; not a one-to-many merge\nDuplicates in left:\n  user_id\n       2 ...\n\n\n\nエラーをはいてくれるので結合前に対策ができる"
  },
  {
    "objectID": "work/improve_python/advanced_pandas.html#チェーンメソッド",
    "href": "work/improve_python/advanced_pandas.html#チェーンメソッド",
    "title": "【脱初心者】Pandasのおしゃな書き方を目指して",
    "section": "チェーンメソッド",
    "text": "チェーンメソッド\n\ndf = sns.load_dataset('titanic')\ndf\n\n\n# こんな感じのやつ\ndf_preprocessed = (\n    df[[\"survived\", \"pclass\", \"age\", \"fare\"]]\n    .dropna(subset=['age', 'fare'])\n    .convert_dtypes()\n)\ndf_preprocessed\n\nassignとpipを使えば複雑なものもチェーンメソッドに書ける\n\ndef add_tax(df, tax_rate=0.1):\n    df_copy = df.copy()\n    df_copy['fare_with_tax'] = df_copy['fare'] * (1 + tax_rate)\n    return df_copy\n\ndef hoge(df):\n    return 1\n\n\nadd_entry(\n    category=\"チェーンメソッド\",\n    code=\"df.assign(...)\",\n    purpose=\"チェーンメソッドで新しい列を追加する\",\n    used_data=\"titanic\",\n)\n\nadd_entry(\n    category=\"チェーンメソッド\",\n    code=\"df.pipe(...)\",\n    purpose=\"チェーンメソッドで関数を適用する\",\n    used_data=\"titanic\",\n)\n\n\ndf_preprocessed = (\n    df[[\"survived\", \"pclass\", \"age\", \"fare\"]]\n    .dropna(subset=['age', 'fare'])\n    .convert_dtypes()\n    .assign(age_group=lambda x: pd.cut(x[\"age\"], bins=[0, 12, 20, 40, 60, 80], labels=[\"child\", \"teen\", \"adult\", \"middle_aged\", \"senior\"]))\n    .pipe(add_tax, tax_rate=0.1)\n    .pipe(hoge)\n)\ndf_preprocessed"
  },
  {
    "objectID": "work/improve_python/advanced_pandas.html#要約",
    "href": "work/improve_python/advanced_pandas.html#要約",
    "title": "【脱初心者】Pandasのおしゃな書き方を目指して",
    "section": "要約",
    "text": "要約\n\ndata\n\n\nsummary_df = pd.DataFrame(data)\nsummary_df.to_markdown(\"./data/advanced_pandas_summary.md\", index=False)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tako Data Science Lab",
    "section": "",
    "text": "プロフィール写真\n\n\n名前: 池田 拓哉（Tako）\n所属: 株式会社PREVENT\n職種: データサイエンティスト\n\n\n\n【不登校・退学、そして海外へ】 神戸学院大学経済学部に在学中、将来への漠然とした不安から不登校になり中退。その後、心機一転しシンガポールとマレーシアへ一人旅に出ました。そこで出会った人たちとの交流を通じて「英語を学びたい」と強く思い、アメリカのペンシルベニア州にある Delaware County Community College (DCCC) への留学を決意しました。\n【プログラミングと統計学との出会い】 留学当初はビジネス専攻でしたが、ルームメイトの影響でプログラミングに興味を持ち、Computer Science専攻に変更。ここでC++と統計学に出会ったことが、現在のキャリアの原点です。2019年に同校を卒業後、帰国。\n【キャリアの転換点】 帰国後はフリーターとして活動していましたが、2020年3月に株式会社リクルートR&Dに入社。実務の中で「自分はプログラマーではなく、データサイエンティストになりたい」という目標を見出し、転職を決意しました。\n【データサイエンティストとしての歩み】 2021年11月、コグラフ株式会社に入社。SQL、Python、BIツールを用いた分析実務のほか、ETL構築やマネジメント（部長職）を経験しました。 その後、9ヶ月間のフリーランス期間を経て、より事業の成長と責任にコミットするため、2025年2月に 株式会社PREVENT に入社。現在は医療データ分析に従事しています。\n\n\n\n\n📊 ポートフォリオ\n📝 技術ブログ (Zenn)\n\n\n\n\n\nEmail: ikedat350@gmail.com\nSNS: X (Twitter) / Wantedly"
  },
  {
    "objectID": "index.html#プロフィール",
    "href": "index.html#プロフィール",
    "title": "Tako Data Science Lab",
    "section": "",
    "text": "プロフィール写真\n\n\n名前: 池田 拓哉（Tako）\n所属: 株式会社PREVENT\n職種: データサイエンティスト\n\n\n\n【不登校・退学、そして海外へ】 神戸学院大学経済学部に在学中、将来への漠然とした不安から不登校になり中退。その後、心機一転しシンガポールとマレーシアへ一人旅に出ました。そこで出会った人たちとの交流を通じて「英語を学びたい」と強く思い、アメリカのペンシルベニア州にある Delaware County Community College (DCCC) への留学を決意しました。\n【プログラミングと統計学との出会い】 留学当初はビジネス専攻でしたが、ルームメイトの影響でプログラミングに興味を持ち、Computer Science専攻に変更。ここでC++と統計学に出会ったことが、現在のキャリアの原点です。2019年に同校を卒業後、帰国。\n【キャリアの転換点】 帰国後はフリーターとして活動していましたが、2020年3月に株式会社リクルートR&Dに入社。実務の中で「自分はプログラマーではなく、データサイエンティストになりたい」という目標を見出し、転職を決意しました。\n【データサイエンティストとしての歩み】 2021年11月、コグラフ株式会社に入社。SQL、Python、BIツールを用いた分析実務のほか、ETL構築やマネジメント（部長職）を経験しました。 その後、9ヶ月間のフリーランス期間を経て、より事業の成長と責任にコミットするため、2025年2月に 株式会社PREVENT に入社。現在は医療データ分析に従事しています。\n\n\n\n\n📊 ポートフォリオ\n📝 技術ブログ (Zenn)\n\n\n\n\n\nEmail: ikedat350@gmail.com\nSNS: X (Twitter) / Wantedly"
  },
  {
    "objectID": "demo/improve_python/demo_batch.html",
    "href": "demo/improve_python/demo_batch.html",
    "title": "Python標準ライブラリーでバッチ処理を行う方法",
    "section": "",
    "text": "itertools.batched は、Python 3.12 で導入された「データを指定したサイズごとに分割して（バッチ化して）取り出す」ための非常に便利な関数です。\nこれまで、機械学習のミニバッチ作成や、APIへの小分け送信、巨大なログの分割処理などを行う際は、トリッキーな自作関数やサードパーティ製ライブラリ（more-itertools など）を使う必要がありましたが、それが標準ライブラリだけでスマートに書けるようになりました。\n\nimport pandas as pd\nimport duckdb\nimport faker\n\nfrom itertools import batched\n\n\ndata = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n\n# 3個ずつのバッチに分割\nfor batch in batched(data, 4):\n    print(batch)\n\n# 出力:\n# ('A', 'B', 'C')\n# ('D', 'E', 'F')\n# ('G',)  &lt;- 余った分も最後に出力される\n\n('A', 'B', 'C', 'D')\n('E', 'F', 'G')\n\n\n\ndata = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n\n# 3個ずつのバッチに分割\nfor batch in batched(data, 3, strict=True):\n    print(batch)\n\n# 出力:\n# ('A', 'B', 'C')\n# ('D', 'E', 'F')\n# ('G',)  &lt;- 余った分も最後に出力される\n\n('A', 'B', 'C')\n('D', 'E', 'F')\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[3], line 4\n      1 data = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n      3 # 3個ずつのバッチに分割\n----&gt; 4 for batch in batched(data, 3, strict=True):\n      5     print(batch)\n      7 # 出力:\n      8 # ('A', 'B', 'C')\n      9 # ('D', 'E', 'F')\n     10 # ('G',)  &lt;- 余った分も最後に出力される\n\nValueError: batched(): incomplete batch\n\n\n\n\n# fakerを使って100万行のダミーデータを生成し、DuckDBに挿入\nfake = faker.Faker()\n\n\n%%time\n\ndata = [{\n    \"name\":fake.name(),\n    \"sex\":fake.random_element(elements=[\"M\", \"F\"]),\n    \"age\":fake.random_int(min=18, max=80),\n    \"address\":fake.address().replace(\"\\n\", \", \"),\n    \"phone_number\":fake.phone_number(),\n} for i in range(1_000_000)\n]\n\ndf = pd.DataFrame(data)\n# display(df.head())\n# df.shape\n\nCPU times: total: 2min 2s\nWall time: 2min 2s\n\n\n\n%%time\n\n# ジェネレータ関数で無限にデータを生成できるようにしておく\ndef fake_data_generator():\n    while True:\n        yield {\n            \"name\":fake.name(),\n            \"sex\":fake.random_element(elements=[\"M\", \"F\"]),\n            \"age\":fake.random_int(min=18, max=80),\n            \"address\":fake.address().replace(\"\\n\", \", \"),\n            \"phone_number\":fake.phone_number(),\n        }\n\nall_dfs = []\nfor batch in batched(fake_data_generator(), 10000): # 1万件ずつバッチ処理\n    batch_df = pd.DataFrame(batch)\n    all_dfs.append(batch_df)\n    \n    if len(all_dfs) &gt;= 100: # 10000 * 100 = 1_000_000件でストップ\n        break\n\ndf = pd.concat(all_dfs)\n\nCPU times: total: 2min 6s\nWall time: 2min 6s\n\n\n\noutput_csv = \"fake_data_100k.csv\"\ndf.to_csv(output_csv, index=False)\n\n\n%%timeit\nreader = pd.read_csv(output_csv, chunksize=10000)\n\ndf_tmps = []\nfor df_chunk in reader:\n    df_tmps.append(df_chunk)\ndf_tmp = pd.concat(df_tmps)\n\n# df_tmp.head()\n\n108 ms ± 737 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n%%timeit\ndf_tmp = pd.read_csv(output_csv)\n\n82.1 ms ± 817 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nimport time\n\nuser_ids = range(1000)\nfor batch in batched(user_ids, 50, strict=False):\n    print(batch)\n    time.sleep(1) # 負荷軽減のための待機\n\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49)\n(50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99)\n(100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149)\n(150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199)\n(200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249)\n(250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299)\n(300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349)\n(350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399)\n(400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449)\n(450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499)\n(500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549)\n(550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599)\n(600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649)\n(650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699)\n(700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749)\n(750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799)\n(800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849)\n(850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899)\n(900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949)\n(950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999)"
  },
  {
    "objectID": "demo.html",
    "href": "demo.html",
    "title": "Lab",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\nNumPyroを使ってベイズ回帰モデルの実装\n\n\n\n統計手法\n\nベイズ推論\n\n\n\nベイズ回帰モデルの実装をNumPyroを使って行う\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\nSARIMAを使った医療費予測のデモ\n\n\n\n統計手法\n\n時系列モデル\n\n\n\nSARIMA（Seasonal AutoRegressive Integrated Moving Average）は、季節性のある時系列データに適した予測モデルです。医療費の階差を取ると、定常分布に近づき、医療費は冬はインフルエンザが流行したり夏は熱中症が増えたりと季節性があるため、SARIMAモデルを使用して医療費の予測を行います。\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\n基本的な因子分析のデモ\n\n\n\n統計手法\n\n因子分析\n\n\n\n因子分析の基本的な実装をfactor_analyzerを使って行う\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\nPython標準ライブラリーでバッチ処理を行う方法\n\n\n\nPython基礎\n\n標準ライブラリ\n\n\n\nitertools.batched は、Python 3.12 で導入された「データを指定したサイズごとに分割して（バッチ化して）取り出す」ための非常に便利な関数です。これまで、機械学習のミニバッチ作成や、APIへの小分け送信、巨大なログの分割処理などを行う際は、トリッキーな自作関数やサードパーティ製ライブラリ（more-itertools など）を使う必要がありましたが、それが標準ライブラリだけでスマートに書けるようになりました。\n\n\n\n\n\nJan 31, 2026\n\n\n\n\n\n\n\nT-stringの実演\n\n\n\nPython基礎\n\n標準ライブラリ\n\n\n\nこの機能は、従来の f-strings と見た目は似ていますが、「文字列をすぐに作らず、構造データとして保持する」という点が決定的に違います。まだuvの環境ではできない。。\n\n\n\n\n\nJan 31, 2026\n\n\n\n\n\n\n\nSteamのオープンデータを使ったEDA\n\n\n\nAnalysis\n\nSteam\n\n\n\nSteamのオープンデータを使ったEDAを実施し、売れ筋のカテゴリなどを分析する\n\n\n\n\n\nFeb 1, 2026\n\n\n\n\n\n\n\n【脱初心者】Pandasのおしゃな書き方を目指して\n\n\n\nPython基礎\n\nPandas\n\n\n\nPandasの機能をもうちょっといい感じに使うために中級上級者になるため\n\n\n\n\n\nFeb 7, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "work/analysis/steam-analysis/script/steam_data_extraction.html",
    "href": "work/analysis/steam-analysis/script/steam_data_extraction.html",
    "title": "Steamのオープンデータを使ったEDA",
    "section": "",
    "text": "import os\nimport requests\nimport pandas as pd\n\n\n# api_key = os.environ.get(\"API_KEY\")\napi_key = \"__\"\n\nprint(api_key)\n\n\ndef GetOwnedGames(api_key, uid):\n    url = \"http://api.steampowered.com/IPlayerService/GetOwnedGames/v1/?key={}&steamid={}&format=json\".format(\n        api_key, uid\n    )\n    r = requests.get(url)\n    data = json.loads(r.text)\n    return data[\"response\"]"
  },
  {
    "objectID": "work/improve_python/data/advanced_pandas_summary.html",
    "href": "work/improve_python/data/advanced_pandas_summary.html",
    "title": "Tako Data Science Lab",
    "section": "",
    "text": "カテゴリ\nコード\n用途\n使用データ\n\n\n\n\n読込\npd.read_csv(path, chunksize=100, usecols=[…])\n大容量のCSVファイルをメモリを節約しながら読み込む\ntitanic\n\n\n出力\ndf.to_markdown(‘path’)\nデータをマークダウン形式で出力する\ntitanic, survey, users, orders\n\n\n出力\ndf.style.background_gradient(path)\nデータをヒートマップで可視化する\ntitanic\n\n\n抽出\ndf.nlargest(n=10, columns=‘fare’)\n指定したカラムのデータを大きい順に抽出する\ntitanic\n\n\n抽出\ndf.nsmallest(n=10, columns=‘fare’)\n指定したカラムのデータを小さい順に抽出する\ntitanic\n\n\n抽出\ndf.select_dtypes(include=‘データタイプ’)\n特定のデータ型のカラムのみを抽出する\ntitanic\n\n\nデータ型\nconvert_dtypes()\nデータ型を柔軟に変換する\ntitanic\n\n\n整形\npd.cut(Series, bins=[], labels=[])\n範囲を指定して数値データをカテゴリ変数に変換する\ntitanic\n\n\n整形\npd.qcut(Series, q=4, labels=[])\n分割数を決めて数値データをカテゴリ変数に変換する\ntitanic\n\n\n整形\ndf.explode(column_name)\nリスト型のデータを展開して縦長にする\nsurvey\n\n\n整形\ndf.explode(column_name)\nリスト型のデータを展開して縦長にする\ntitanic\n\n\n整形\ndf.melt(id_vars=[], var_name=’‘, value_name=’’)\nデータを横長から縦長に変換する\ntitanic\n\n\n結合\ndf.merge(other_df, on=‘column_name’, how=‘inner’, validate=‘one_to_many’)\n結合前にデータの関係性を検証する\nusers, orders\n\n\nチェーンメソッド\ndf.assign(…)\nチェーンメソッドで新しい列を追加する\ntitanic\n\n\nチェーンメソッド\ndf.pipe(…)\nチェーンメソッドで関数を適用する\ntitanic"
  },
  {
    "objectID": "work/improve_python/data/survey.html",
    "href": "work/improve_python/data/survey.html",
    "title": "Tako Data Science Lab",
    "section": "",
    "text": "ユーザーID\n年齢\n満足度\n好きな食べ物\n\n\n\n\nU001\n32\n2\n[‘寿司’, ‘カレー’, ‘ラーメン’]\n\n\nU002\n40\n5\n[‘ラーメン’]\n\n\nU003\n29\nnan\n[‘サラダ’, ‘寿司’, ‘パスタ’]\n\n\nU004\n49\n4\n[‘ハンバーグ’]\n\n\nU005\n39\n5\n[‘パスタ’, ‘ラーメン’, ‘ハンバーグ’]\n\n\nU006\n47\n4\n[‘寿司’, ‘ハンバーグ’]\n\n\nU007\n57\nnan\n[‘カレー’, ‘ラーメン’]\n\n\nU008\n20\n2\n[‘ラーメン’, ‘パスタ’, ‘カレー’]\n\n\nU009\n31\n2\n[‘ハンバーグ’, ‘寿司’, ‘ラーメン’]\n\n\nU010\n33\n4\n[‘パスタ’, ‘寿司’, ‘ラーメン’]"
  },
  {
    "objectID": "work/improve_python/data/users.html",
    "href": "work/improve_python/data/users.html",
    "title": "Tako Data Science Lab",
    "section": "",
    "text": "user_id\nuser_name\n\n\n\n\n1\n田中\n\n\n2\n佐藤_旧\n\n\n2\n佐藤_新\n\n\n3\n鈴木"
  },
  {
    "objectID": "work/improve_python/demo_tstring.html",
    "href": "work/improve_python/demo_tstring.html",
    "title": "T-stringの実演",
    "section": "",
    "text": "# Templateオブジェクトのインポート\nfrom string.templatelib import Template\n\n\nname = \"Python\"\n# f-string: すぐに \"Hello Python\" という文字列になる\nf_res = f\"Hello {name}\" \n\n# t-string: 構造を保持したオブジェクトになる\nt_res = t\"Hello {name}\"\n\nprint(type(t_res)) # &lt;class 'string.templatelib.Template'&gt;\nprint(t_res.strings) # ('Hello ', '')\nprint(t_res.values)  # ('Python',)"
  },
  {
    "objectID": "work/statistic_methods/basic_factor_analyze/factor_analyze_demo.html",
    "href": "work/statistic_methods/basic_factor_analyze/factor_analyze_demo.html",
    "title": "基本的な因子分析のデモ",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom factor_analyzer import FactorAnalyzer\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\nfrom factor_analyzer.factor_analyzer import calculate_kmo\n\n# グラフのスタイル設定\nsns.set(style='whitegrid')\n# 注意: 日本語フォント設定は環境に合わせて調整してください（Colabの場合は別途インストールが必要）\n# ここでは英語ラベルで進行しますが、意味は解説します。\n\n\nダミーデータの作成\n因子分析が綺麗に決まるよう、意図的に相関を持たせた健診データを作成します。\nここでは2つの潜在因子（1. 代謝・運動不足因子、2. ストレス・血圧因子）を想定してデータを生成します。\n\n# 乱数シードの固定\nnp.random.seed(42)\nn_samples = 500\n\n# 潜在変数（真の要因）を作成\n# Factor 1: 運動不足・肥満傾向 (Metabolic)\nf_metabolic = np.random.normal(0, 1, n_samples)\n# Factor 2: ストレス・交感神経緊張 (Stress)\nf_stress = np.random.normal(0, 1, n_samples)\n\n# 観測変数（健診項目）を作成\n# 式: 観測値 = 負荷量 * 因子 + ノイズ\ndata = {\n    # --- 因子1（代謝・肥満）の影響が強い項目 ---\n    'BMI': 0.8 * f_metabolic + 0.1 * f_stress + np.random.normal(0, 0.5, n_samples),\n    '腹囲(Waist)': 0.85 * f_metabolic + 0.1 * f_stress + np.random.normal(0, 0.5, n_samples),\n    '中性脂肪(TG)': 0.7 * f_metabolic + 0.2 * f_stress + np.random.normal(0, 0.6, n_samples),\n    '空腹時血糖(FPG)': 0.6 * f_metabolic + 0.3 * f_stress + np.random.normal(0, 0.6, n_samples),\n    'HDLコレステロール': -0.7 * f_metabolic + np.random.normal(0, 0.6, n_samples), # 逆相関\n\n    # --- 因子2（ストレス・血圧）の影響が強い項目 ---\n    '収縮期血圧(SBP)': 0.2 * f_metabolic + 0.85 * f_stress + np.random.normal(0, 0.5, n_samples),\n    '拡張期血圧(DBP)': 0.2 * f_metabolic + 0.80 * f_stress + np.random.normal(0, 0.5, n_samples),\n    '脈拍(Pulse)': 0.1 * f_metabolic + 0.6 * f_stress + np.random.normal(0, 0.7, n_samples),\n}\n\ndf = pd.DataFrame(data)\n\n# データの確認\nprint(\"データの相関行列（一部）:\")\ndisplay(df.corr().round(2))\n\nデータの相関行列（一部）:\n\n\n\n\n\n\n\n\n\nBMI\n腹囲(Waist)\n中性脂肪(TG)\n空腹時血糖(FPG)\nHDLコレステロール\n収縮期血圧(SBP)\n拡張期血圧(DBP)\n脈拍(Pulse)\n\n\n\n\nBMI\n1.00\n0.72\n0.62\n0.59\n-0.63\n0.21\n0.20\n0.16\n\n\n腹囲(Waist)\n0.72\n1.00\n0.63\n0.60\n-0.64\n0.19\n0.18\n0.13\n\n\n中性脂肪(TG)\n0.62\n0.63\n1.00\n0.55\n-0.52\n0.32\n0.30\n0.20\n\n\n空腹時血糖(FPG)\n0.59\n0.60\n0.55\n1.00\n-0.50\n0.40\n0.35\n0.28\n\n\nHDLコレステロール\n-0.63\n-0.64\n-0.52\n-0.50\n1.00\n-0.11\n-0.07\n-0.05\n\n\n収縮期血圧(SBP)\n0.21\n0.19\n0.32\n0.40\n-0.11\n1.00\n0.74\n0.56\n\n\n拡張期血圧(DBP)\n0.20\n0.18\n0.30\n0.35\n-0.07\n0.74\n1.00\n0.53\n\n\n脈拍(Pulse)\n0.16\n0.13\n0.20\n0.28\n-0.05\n0.56\n0.53\n1.00\n\n\n\n\n\n\n\n\n\n因子分析の事前検定\nデータが因子分析に適しているかを確認します（KMO検定とバートレット検定）\n\n# 1. バートレットの球面性検定\n# p値が0.05未満なら、相関行列は単位行列ではない（因子分析する意味がある）\nchi_square_value, p_value = calculate_bartlett_sphericity(df)\nprint(f\"Bartlett's test p-value: {p_value:.3e}\")\n\n# 2. KMO (Kaiser-Meyer-Olkin) 検定\n# 0.6以上であれば因子分析に適しているとされる\nkmo_all, kmo_model = calculate_kmo(df)\nprint(f\"KMO Test Value: {kmo_model:.3f}\")\n\nif kmo_model &gt; 0.6 and p_value &lt; 0.05:\n    print(\"&gt;&gt; 判定: このデータは因子分析に適しています。\")\nelse:\n    print(\"&gt;&gt; 判定: データを見直す必要があるかもしれません。\")\n\nBartlett's test p-value: 0.000e+00\nKMO Test Value: 0.846\n&gt;&gt; 判定: このデータは因子分析に適しています。\n\n\n\n\n因子の数を決める（スクリープロット）\n固有値を計算し、いくつの因子を抽出・保持すべきかをグラフで確認します。\n\n# 因子数を変数の数だけ設定して仮実行\nfa = FactorAnalyzer(n_factors=len(df.columns), rotation=None)\nfa.fit(df)\n\n# 固有値（Eigenvalues）の取得\nev, v = fa.get_eigenvalues()\n\n# スクリープロットの描画\nplt.figure(figsize=(8, 5))\nplt.scatter(range(1, df.shape[1]+1), ev)\nplt.plot(range(1, df.shape[1]+1), ev)\nplt.title('Scree Plot')\nplt.xlabel('Number of Factors')\nplt.ylabel('Eigenvalue')\nplt.axhline(y=1, color='r', linestyle='--') # 基準線（固有値1以上を採用することが多い）\nplt.grid(True)\nplt.show()\n\nprint(\"固有値:\", ev.round(2))\n# 通常、固有値が急激に下がってなだらかになる直前、または1以上の数を選びます。\n# 今回の設計では「2」または「3」あたりで折れるはずです。\n\n\n\n\n\n\n\n\n固有値: [3.83 1.89 0.53 0.44 0.43 0.35 0.28 0.25]\n\n\n\n\n因子分析の実行とヒートマップ可視化\n因子数を「2」と仮定して実行します。 回転法には promax（斜交回転）を使用します。これは、健康要因同士（肥満とストレス）には相関があることが自然だからです。\n\nimport plotly.express as px\n\n# --- 分析パート（前回と同じ）---\n# 因子の数を指定（スクリープロットの結果を見て 2 と仮定）\nn_factors = 2\n\n# 因子分析の実行 (promax回転)\nfa = FactorAnalyzer(n_factors=n_factors, rotation='promax')\nfa.fit(df)\n\n# 因子負荷量（Factor Loadings）の取得\nloadings = pd.DataFrame(fa.loadings_, \n                        index=df.columns, \n                        columns=[f'Factor{i+1}' for i in range(n_factors)])\n\n# --- 可視化パート (Plotly) ---\nfig_heatmap = px.imshow(\n    loadings,\n    text_auto='.2f',  # 値を小数点2桁で表示\n    aspect=\"auto\",\n    color_continuous_scale='RdBu_r', # 赤〜青（相関の正負を表現）\n    zmin=-1, zmax=1,\n    title='Factor Loadings (因子負荷量)'\n)\n\nfig_heatmap.update_layout(\n    xaxis_title=\"Factors\",\n    yaxis_title=\"Variables\",\n    width=600,\n    height=600\n)\n\nfig_heatmap.show()\n\n# 因子の解釈ヒント\nprint(\"--- 因子の解釈 ---\")\nprint(\"赤色が強い箇所 = その因子との正の相関が強い\")\nprint(\"青色が強い箇所 = その因子との負の相関が強い\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n--- 因子の解釈 ---\n赤色が強い箇所 = その因子との正の相関が強い\n青色が強い箇所 = その因子との負の相関が強い\n\n\n\n\n因子得点の算出（個人のスコアリング）\n\nimport plotly.graph_objects as go\n\n# --- 計算パート ---\n# 因子得点の計算\nfactor_scores = pd.DataFrame(fa.transform(df), \n                            columns=[f'Factor{i+1}' for i in range(n_factors)])\n\n# 元データと結合\nresult_df = pd.concat([df, factor_scores], axis=1)\n\n# 因子の命名（ヒートマップの結果を見て適宜変更してください）\n# ここでは Factor1=代謝リスク, Factor2=ストレス・血圧リスク と仮定\nx_axis_label = 'Score_Metabolic(運動不足)'\ny_axis_label = 'Score_Stress(高血圧負荷)'\n\nresult_df = result_df.rename(columns={\n    'Factor1': x_axis_label,\n    'Factor2': y_axis_label\n})\n\n# --- 可視化パート (Plotly) ---\nfig_scatter = px.scatter(\n    result_df, \n    x=x_axis_label, \n    y=y_axis_label,\n    # ホバーした時に元の健診データを表示（分析に便利！）\n    hover_data=['BMI', '収縮期血圧(SBP)', '空腹時血糖(FPG)'], \n    title='受診者の因子スコア分布（リスクマップ）',\n    opacity=0.7\n)\n\n# 中心線（0,0）を追加して象限をわかりやすくする\nfig_scatter.add_vline(x=0, line_width=1, line_dash=\"dash\", line_color=\"gray\")\nfig_scatter.add_hline(y=0, line_width=1, line_dash=\"dash\", line_color=\"gray\")\n\n# レイアウト調整\nfig_scatter.update_layout(\n    xaxis_title=x_axis_label,\n    yaxis_title=y_axis_label,\n    width=800,\n    height=600,\n    template='plotly_white'\n)\n\n# 象限の注釈（必要に応じて）\nfig_scatter.add_annotation(x=2, y=2, text=\"高リスク群\", showarrow=False, font=dict(color=\"red\"))\nfig_scatter.add_annotation(x=-2, y=-2, text=\"健康群\", showarrow=False, font=dict(color=\"green\"))\n\nfig_scatter.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nノート\n\n因子負荷量 (Loading): 変数と因子の相関係数のようなもの。絶対値が0.4以上あれば関係が強いと見なします。\n回転 (Rotation): promax は斜交回転で、因子間の相関を許容します（現実的）。varimax は直交回転で、因子間が無相関であると仮定します。\n解釈: 出力されたヒートマップを見て、「因子1はBMIと血糖値が高いから『メタボ因子』だな」と人間が意味付けを行います。"
  }
]